---
title: "Topic Analysis of tweets belonging to spatial clusters"
author: "Marcela"
date: "September 1, 2019"
output: html_document
---


<br>
```{r import_data, echo=FALSE, message=FALSE}
# setwd("/home/marcela/Coding/MarcesThesis/MarcesThesis/")
colo_clean <- readRDS(file = "colo_clean.rds")
```



#### **Data**

Now I will repeat the same analysis but considering tweets sent from what should be the "affected area" by using spatial clustering.

Before doing so, the data was first projected in North America Lambert Conformal Conic.

```{r getting_spatial, echo=FALSE, message=FALSE}

library(sf)

# Store tweets as simple features and project data
colo_sf_p <- colo_clean %>% 
  st_as_sf(coords = c("lon", "lat"), crs = 4326)


colo_sf <- colo_sf_p %>% # set WGS84 as original datum
  st_transform(crs ="+proj=lcc +lat_1=20 + lat_2=60 + lat_0=40 +
               lon_0=-96 +x_0=0 +y_0=0 +ellps=GRS80 +datum=NAD83 +
               units=m no_defs") # Projected in North_America_Lambert_Conformal_Conic

st_crs(colo_sf) # Retrieve current coord ref system: EPSG: 4326 WGS84

```

<br>

####  **Subsetting by Spatial Clustering**

In order to identify tweets sent from the afected area, a hierarchical implementation of dbscan was used **(hdscan)**, considering a minimum of 350 tweets per cluster.

```{r spatial_clusters, echo=FALSE, message=FALSE}

library(dbscan)
library(leaflet)
library(htmltools)
library(tidyverse)


set.seed(123)

clusters <- hdbscan(colo_sf %>%
                      st_coordinates(), #This rounds coordinates
                    minPts = 550)

colo_clusters <- colo_sf %>% 
  mutate(cluster = clusters$cluster)

## Plotting spatial cluster results

colo_clusters <- colo_clusters %>% # Need to reproject in WGS84 datum. long lat format.
  st_transform(crs = 4326)

colo_clusters$cluster <- as.factor(colo_clusters$cluster) #Clusters as factors for coloring
pal <- colorFactor(c("#636363", "red", "Blue"), domain = c("0", "1", "2"))

# #1. or Red cluster has 608 tweets. Denver
# #2. or Blue cluster has 1998 tweets. Boulder
# 2219/4840 tweets were classified as outliers.
```

```{r map_clusters, echo=FALSE, message=FALSE}

coloMap <- leaflet(colo_clusters) %>% # Interactive map to see resulting clusters
  addTiles()  %>%
  addProviderTiles(providers$OpenStreetMap.BlackAndWhite) %>% 
  addCircles(weight = 3, 
             radius=40,
             color= ~pal(cluster), 
             stroke = TRUE, 
             fillOpacity = 0.5,
             popup = ~htmlEscape(cluster))%>% 
  setView(lng = -105, lat = 40, zoom = 8.5)

coloMap

```



#####  **Most common words (Within Spatial Clusters)**

A quick view of the most common words in the whole dataset: 

```{r data_subsets_tidy, echo=FALSE, message=FALSE}

#This is mainly to remove geometry to make computation faster

clusters_df <- colo_clusters %>% 
  filter(cluster == 1 | cluster == 2) %>% 
  mutate(tweet = text) %>% # To keep a column with the original tweet
  st_set_geometry(NULL)

```

```{r one-token-per-document-per-row-clusters, echo=FALSE, message=FALSE}
library(tidytext)

clusters_tidy <- clusters_df %>% 
  unnest_tokens(word, text, token = "tweets") %>% # add a row for each token (word) and repeat the other information
  anti_join(stop_words)  # remove stopwords
  
clusters_tidy %>% 
  count(word, sort = TRUE) # Count words

```


Again, since **"boulder"** is the most common word and is going to have a big effect in our topic modelling, it was removed from the dataset. After this, the new list of common words looks as follows:
                                 
                                 
```{r remove_words_clusters, echo=FALSE, message=FALSE}

keywords <- c("boulder", "amp", "|")


clusters_tidy <- clusters_df %>% 
  unnest_tokens(word, text, token = "tweets") %>% # add a row for each token (word) and repeat the other information
  anti_join(stop_words) %>%   # remove stopwords
  filter(!word %in% keywords) 

clusters_tidy %>% 
  count(word, sort = TRUE) # Count words

```


```{r explore_tf_idf_clusters, echo=FALSE, message=FALSE}

clusters_tf_idf <- clusters_tidy %>% 
  count(stage, word, sort = TRUE) %>% 
  bind_tf_idf(word, stage, n) %>%
  arrange(-tf_idf) %>%
  group_by(stage) %>% 
  top_n(9) %>% 
  ungroup 

clusters_tf_idf %>% 
  mutate(word = reorder_within(word, tf_idf, stage)) %>% #reordering by tf_idf
  ggplot(aes(word, tf_idf, fill = stage)) +
  geom_col(alpha = 0.8, show.legend = FALSE) +
  facet_wrap(~stage, scales = "free", ncol = 2)+
  scale_x_reordered() +
  coord_flip() +
  theme(strip.text=element_text(size=11)) +
  labs(x = NULL, y = "tf-idf",
       title = "Highest tf-idf words in tweets during the different stages of Colorado Flood",
       subtitle = "Words importance within tweets sent during each stage")

```

<br>



#####  **Topic Modeling**

Again, after playing with different numbers, I decided to train a topic model with 5 topics. From 6 on, topics started to look very similar (with the same bag of words). Here is a summary of the results after this process: 

```{r topic_modeling_clusters, echo=FALSE, message=FALSE}

library(stm) #fast compared to other implementations of topic models. Base don C++
library(quanteda)

# Either one (of the following) works as input for the topic modelling

# Create Document-Term Matrix
clusters_dfm <- clusters_tidy %>%
  count(tweet, word, sort = TRUE) %>% 
  cast_dfm(tweet, word, n) # 4 documents (4 stages). Quanteda document frecuency. Special implementation for document term matrix

# Create Sparce Matrix
clusters_sparse <- clusters_tidy %>%
  count(tweet_id, word) %>%
  cast_sparse(tweet_id, word, n)

saveRDS(clusters_sparse, file = "clusters_sparse.rds")
```


```{r plot_topic_modeling_results, echo=FALSE, message=FALSE, fig.width=10, fig.height=7}

k_result_clusters <- readRDS(file = "k_result_clusters.rds")

k_result_clusters %>%
  transmute(K,
            `Lower bound` = lbound,
            Residuals = map_dbl(residual, "dispersion"),
            `Semantic coherence` = map_dbl(semantic_coherence, mean),
            `Held-out likelihood` = map_dbl(eval_heldout, "expected.heldout")) %>%
  gather(Metric, Value, -K) %>%
  ggplot(aes(K, Value, color = Metric)) +
  geom_line(size = 1.5, alpha = 0.7, show.legend = FALSE) +
  facet_wrap(~Metric, scales = "free_y") +
  labs(x = "K (number of topics)",
       y = NULL,
       title = "Model diagnostics by number of topics",
       subtitle = "These diagnostics indicate that a good number of topics would be around 21")
```


```{r plot_semantic_and_exclusivity, echo=FALSE, message=FALSE, fig.width=9, fig.height=6}

k_result_clusters <- readRDS(file = "k_result_clusters.rds")

k_result_clusters %>%
  select(K, exclusivity, semantic_coherence) %>%
  filter(K %in% c(7,12,18,21,27,29)) %>% # This values should be the same used when creating the model in the R script
  unnest() %>%
  mutate(K = as.factor(K)) %>%
  ggplot(aes(semantic_coherence, exclusivity, color = K)) +
  geom_point(size = 2, alpha = 0.7) +
  labs(x = "Semantic coherence",
       y = "Exclusivity",
       title = "Comparing exclusivity and semantic coherence",
       subtitle = "Models with fewer topics have higher semantic coherence for more topics, but lower exclusivity")

```

```{r beta, echo=FALSE, message=FALSE, fig.width=10, fig.height=8}
topic_model <- k_result_clusters %>% 
  filter(K == 21) %>% 
  pull(topic_model) %>% 
  .[[1]]

# topic_model

td_beta_clusters <- tidy(topic_model) # The beta matrix shows what are the words that contribute to each topic.

td_beta_clusters %>% 
  group_by(topic) %>% 
  top_n(10, beta) %>% 
  ungroup %>% 
  mutate(topic = paste0("Topic ", topic),
         term = reorder(term, beta)) %>% #reordering by tf_idf
  ggplot(aes(term, beta, fill = as.factor(topic))) +
  geom_col(alpha = 0.8, show.legend = FALSE) +
  facet_wrap(~topic, scales = "free_y") +
  coord_flip() +
  scale_x_reordered() +
  labs(x = NULL, y = expression(beta),
       title = "Highest word probabilities for each topic",
       subtitle = "Different words are associated with different topics") # To see which words contribute the most to each topic
```


```{r gamma, echo=FALSE, message=FALSE, warning=FALSE, fig.width=8, fig.height=8}
library(ggthemes)
library(extrafont)
library(scales)

td_gamma_clusters <- tidy(topic_model, matrix = "gamma",                    
                 document_names = rownames(clusters_sparse))

top_terms_clusters <- td_beta_clusters %>%
  arrange(beta) %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  arrange(-beta) %>%
  select(topic, term) %>%
  summarise(terms = list(term)) %>%
  mutate(terms = map(terms, paste, collapse = ", ")) %>% 
  unnest()

gamma_terms_clusters <- td_gamma_clusters %>%
  group_by(topic) %>%
  summarise(gamma = mean(gamma)) %>%
  arrange(desc(gamma)) %>%
  left_join(top_terms_clusters, by = "topic") %>%
  mutate(topic = paste0("Topic ", topic),
         topic = reorder(topic, gamma))

gamma_terms_clusters %>%
  top_n(21, gamma) %>%
  ggplot(aes(topic, gamma, label = terms, fill = topic, alpha = 0.7)) +
  geom_col(show.legend = FALSE) +
  geom_text(hjust = 0, nudge_y = 0.0005, size = 3,
            family = "CM Roman") +
  coord_flip() +
  scale_y_continuous(expand = c(0,0),
                     limits = c(0, 0.30),
                     labels = percent_format()) +
  theme_tufte(base_family = "CM Roman",ticks = FALSE) +
  theme(plot.title = element_text(size = 14,
                                  family="CM Roman"),
        plot.subtitle = element_text(size = 11),
        legend.position = "none") +
  labs(x = NULL, y = expression(gamma),
       title = "Topics by prevalence in the dataset containing tweets belonging to the spatial clusters",
       subtitle = "With the top words that contribute to each topic")

```

```{r prepare_heatmap, echo=FALSE, message=FALSE}

heat_clusters <- td_gamma_clusters %>% 
  rename(tweetid = document) %>% 
  mutate(tweet_id = as.integer(tweetid)) %>% 
  left_join(colo_clean, by = "tweet_id") %>% 
  select(tweet_id, topic, gamma, day)

# Assigning a topic to each tweet based on which one was had the maximum Gamma
heat_clusters <- heat_clusters %>% 
  group_by(tweet_id) %>% 
  filter(gamma == max(gamma)) %>% 
  ungroup()

# Compute the total number of tweets belonging to each topic each day
heat_clusters <- heat_clusters %>% 
  group_by(day, topic) %>% 
  add_count() %>% 
  rename(total_topic_day = n) %>% 
  ungroup()

# Compute the total number of tweets per day
heat_clusters <- heat_clusters %>% 
  group_by(day) %>% 
  add_count() %>% 
  rename(total_tweets_day = n) %>% 
  mutate(dens = total_topic_day/total_tweets_day)

# Standarazing tweet density (total_topic_day). Computing Z-score values for total_topic_day
scale_this <- function(x){
  (x - mean(x, na.rm=TRUE)) / sd(x, na.rm=TRUE)
}

scaled_heat_clusters <- 
  heat_clusters %>%
  group_by(day) %>%
  mutate(scaled_total_topic_day = scale_this(total_topic_day))

```


```{r gammaclusters2_heatplot, echo=FALSE, message=FALSE}
#Assign color variables
col1 = "#d8e1cf" 
col2 = "#438484"


library(RColorBrewer)
ggplot(scaled_heat_clusters, aes(day, as.factor(topic))) +
  geom_tile(aes(fill = scaled_total_topic_day), color = "white") +
  scale_fill_gradient(low = col1, high = col2) +  
  ylab("Topic") +
  xlab("Flood day") +
  theme(legend.title = element_text(size = 10),
        legend.text = element_text(size = 12),
        plot.title = element_text(size=16),
        axis.title=element_text(size=12,face="bold"),
        axis.text.x = element_text(angle = 90, hjust = 1)) +
  labs(fill = "")
```


