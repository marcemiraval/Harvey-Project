---
title: "Colorado"
author: "Marcela Su√°rez"
date: "July 23, 2019"
output: html_document
---
<br>


This document presents the results obtained for Colorado.  

<br>

#### **A general  overview of the dataset**

```{r import_data, echo=FALSE, message=FALSE}

library(tidyverse) 
colorado <- read_csv("ColoradoFloodOriginal.csv") # Loading data into R

 # Create a subset with the variables to be used
library(lubridate) # For dates
colorado <- colorado %>% 
  mutate(date = ymd_hms(colorado$created_at, tz='America/Denver')) %>% # Mountain Daylight Time. Time zone in Colorado (GMT-6)
  select(lat = latitude, 
         lon = longitude, 
         date = date, 
         state = c_state,
         text = t_text,
         user = u_name,
         u_profile = u_description)

str(colorado) # shows the structure of the dataframe
```

Tweets were collected from `r min(colorado$date)` to `r max(colorado$date)`  

<br>


####  **Temporal stages**

Four stages were defined:

```{r define_temporal_stages, echo=FALSE, message=FALSE}

colorado <- colorado %>%  # The way to define them in a tidyway
  mutate(stage = ifelse(date <= "2013-09-11 00:00:00 MDT", "preflood", 
                        ifelse(date <= "2013-09-16 00:00:00 MDT", "flood",
                               ifelse(date <= "2013-09-23 00:00:00 MDT", "immediate_aftermath", "postflood")))) %>%
    mutate(stage = factor(stage, levels = unique(stage))) # Turning stages into factors to use them for plotting later
```

* *Preflood*: From `r min(colorado$date)` to 2013-09-11 00:00:00.
* *Flood*: From 2013-09-11 00:00:00 to 2013-09-16 00:00:00.
* *Immediate Aftermath*: From 2013-09-16 00:00:00 to 2013-09-23 00:00:00.
* *Postflood*: From 2013-09-23 00:00:00 to `r max(colorado$date)`.

<br>


####  **Cleaning dataset**

* *Removing retweets:*
```{r remove_retweets, echo=FALSE, message=FALSE}

library(tidytext)
library(stringr)

colorado <- colorado %>% 
  filter(!str_detect(text, "^RT")) %>% 
  filter(!str_detect(text, "#TweetMyJobs")) %>% 
  filter(!str_detect(text, "#Job")) %>% 
  filter(!str_detect(text, "I'm")) # There will remain some retuits since those will be commented retweets (actually adding some info)
```
    From the original dataset containing 5915 tweets, only 5846 were retained.  

* *Removing tweets sent by bots:*
```{r remove_bots, echo=FALSE, message=FALSE}
colorado <- colorado %>% 
  filter(!str_detect(u_profile, "TwitZip")) %>% 
  filter(!str_detect(u_profile, "TMJ-COB PM Jobs")) %>% 
  filter(!str_detect(u_profile, "JobsDirectUSA.com")) %>% 
  filter(!str_detect(u_profile, "Whole Foods"))

```
    Tweets sent by [TwitZIP](http://twitzip.com/), a bot service to discover local neighborhood news, were removed. After this process 4916 were retained. Other three bot accounts *("TMJ-COB PM Jobs", "JobsDirectUSA.com", "Whole Foods")* were detected and tweets were removed. A total of 4158 tweets were retained.
    
* *Removing urls, usernames, punctuation and numbers:*
```{r clean_tweets, echo=FALSE, message=FALSE}

colo_clean <- colorado %>% 
   mutate(text = str_remove_all(text, 'http[^ ]+'), #Finally removing urls in a nice way
          text = str_remove_all(text, '@[^ ]+'), # Removing usernames
          text = str_remove_all(text, "[:punct:]"), # Removing puntuation
          text = str_remove_all(text, "[:digit:]")) # Removing numbers
```
    All references to usernames, urls and punctuation and numbers were removed from each tweet in order to prepare the dataset for tokenization.

<br>



####  **Most common words**

A quick view of the most common words in the whole dataset: 

```{r one-token-per-document-per-row, echo=FALSE, message=FALSE}

colo_tidy <- colo_clean %>% 
  mutate(tweet = text) %>% # To keep a column with the original tweet
  unnest_tokens(word, text, token = "tweets") %>% # add a row for each token (word) and repeat the other information
  anti_join(stop_words) # remove stopwords

colo_tidy %>% 
  count(word, sort = TRUE) # Count words
```

Since **"boulder"** is the most common word and is going ot have a big effect in our topic modelling, it was removed from the dataset. The following five terms *("boulderflood", "colorado", "coflood", "cowx", "flood")* were also excluded because they were so common and used neutrally in all four stages. After excluding these six terms, the new list of common words looks as follows:
                                 
                                 
```{r remove_words, echo=FALSE, message=FALSE}

keywords <- c("boulder", "boulderflood", "colorado", "coflood", "cowx", "flood")
colo_tidy <- colo_clean %>% 
  mutate(tweet = text) %>% # To keep a column with the original tweet
  unnest_tokens(word, text, token = "tweets") %>% # add a row for each token (word) and repeat the other information
  anti_join(stop_words) %>%  # remove stopwords
  filter(!word %in% keywords) # Next two candidates to be removed are Im and boulderflood. TO CHECK LATER!

colo_tidy %>% 
  count(word, sort = TRUE) # Count words
```

<br>

The statistic **tf-idf** was computed in order to identify which words are important in each of the flood stages. It measures how important a word is to a document in a collection (or corpus) of documents, in our case, it measures how important a word is to a tweet in a collection of tweets. We explore the collection of tweets belonging to each stage.

```{r explore_tf_idf, echo=FALSE, message=FALSE}

colo_tf_idf <- colo_tidy %>% 
  count(stage, word, sort = TRUE) %>% 
  bind_tf_idf(word, stage, n) %>% 
  group_by(stage) %>% 
  top_n(9) %>% 
  ungroup 

colo_tf_idf %>% 
  mutate(word = reorder(word, tf_idf)) %>% #reordering by tf_idf
  mutate(stage = fct_relevel(stage, "preflood", "flood", "postflood","immediate_aftermath")) %>% # reordering by stages
  ggplot(aes(word, tf_idf, fill = stage)) +
  geom_col(alpha = 0.8, show.legend = FALSE) +
  facet_wrap(~stage, scales = "free") +
  coord_flip()
  
```

<br>



####  **Topic Modeling**

```{r topic_modeling, echo=FALSE, message=FALSE}
library(stm) #fast compared to other implementations of topic models. Base don C++
library(quanteda)

colo_dfm <- colo_tidy %>%
  count(stage, word, sort = TRUE) %>% 
  cast_dfm(stage, word, n) # 4 documents (4 stages). Quanteda document frecuency. Special implementation for document term matrix

# Let's train oir topic model

topic_model <- stm(colo_dfm, K = 4, init.type = "Spectral") # unsupervised 
summary(topic_model)
```
```{r probabilities, echo=FALSE, message=FALSE}
td_beta <- tidy(topic_model) # The beta matrix shows what are the words that contribute to each topic.

td_beta %>% 
  group_by(topic) %>% 
  top_n(10, beta) %>% 
  ungroup %>% 
  mutate(topic = paste0("Topic ", topic),
         term = reorder(term, beta)) %>% #reordering by tf_idf
  ggplot(aes(term, beta, fill = as.factor(topic))) +
  geom_col(alpha = 0.8, show.legend = FALSE) +
  facet_wrap(~topic, scales = "free_y") +
  coord_flip() +
  scale_x_reordered() +
  labs(x = NULL, y = expression(beta),
       title = "Highest word probabilities for each topic",
       subtitle = "Different words are associated with different topics") # To see which words contribute the most to each topic




td_gamma <- tidy(topic_model, matrix = "gamma",                    
                 document_names = rownames(colo_dfm))

ggplot(td_gamma, aes(gamma, fill = as.factor(topic))) +
  geom_histogram(alpha = 0.8, show.legend = FALSE) +
  facet_wrap(~ topic, ncol = 2) +
  labs(title = "Distribution of document probabilities for each topic",
       subtitle = "Each topic is associated with 1-3 stages",
       y = "Number of stages", x = expression(gamma))

```

