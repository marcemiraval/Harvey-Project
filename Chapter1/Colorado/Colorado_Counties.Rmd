---
title: "Colorado_counties"
author: "Marcela Su√°rez"
date: "November 28, 2019"
output: html_document
---
<br>


This document presents the results obtained for Colorado.  

<br>

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo=FALSE, message=FALSE, tidy=TRUE, warning=FALSE)
```

####  **Subsetting by Affected Counties and Flood Phase**

**3858 tweets** sent during the disaster were filtered based on the 9 counties affected in the disaster --according to the reports: Arapahoe, Boulder, Denver, El Paso, Jefferson, Larimer, Logan, Morgan and Weld.

```{r subset_by_county}
library(sf)
library(tidyverse)
library(tidytext)
library(scales)

# library(rgeos) #  rgeos required for finding out which hole belongs to which exterior ring

affected_counties <- readRDS(file = "affected_counties.rds")
# st_crs(affected_counties)

colo_clean <- readRDS(file = "colo_clean.rds")

```

```{r subset_projection}

affected_counties_p <- st_transform(affected_counties, 4326)
# st_crs(affected_counties_p)

```


```{r getting_spatial}

library(sf)

# Store tweets as simple features and project data
colo_sf_p <- colo_clean %>% 
  st_as_sf(coords = c("lon", "lat"), crs = 4326)

# st_crs(colo_sf_p) # Retrieve current coord ref system: EPSG: 4326 WGS84

```


```{r point_in_polygon}
tweet_in_county <- st_join(colo_sf_p, affected_counties_p) %>% 
  select(county_name = NAME,
         date = date,
         state = state,
         text = text,
         user = user,
         u_profile = u_profile,
         stage = stage,
         day = day,
         tweet_id = tweet_id,
         geometry = geometry) %>% 
  filter(!is.na(county_name))

```

Only **2576** tweets sent during the "Flood" stage and from these counties were filtered for content analysis.


```{r point_in_polygon_proj}

tweet_in_county_p <- tweet_in_county %>% st_transform(crs ="+proj=lcc +lat_1=20 + lat_2=60 + lat_0=40 +
               lon_0=-96 +x_0=0 +y_0=0 +ellps=GRS80 +datum=NAD83 +
               units=m no_defs") # Projected in North_America_Lambert_Conformal_Conic

# st_crs(tweet_in_county_p)
```

```{r create_map, echo=FALSE, fig.width=9, fig.height=9}
library(mapview)

mapview(affected_counties_p, zcol = "NAME") +
  mapview(tweet_in_county, 
          lwd = 0.5, 
          cex = 2, 
          popup = leafpop::popupTable(tweet_in_county,
                             zcol = "text"))
```



```{r point_in_polygon_plot}
library(leaflet)

leaflet() %>% # Interactive map to see resulting clusters
  addTiles()  %>%
  addProviderTiles(providers$OpenStreetMap.BlackAndWhite) %>% 
  addPolygons(data = affected_counties) %>% 
  addCircles(data = tweet_in_county,
             weight = 3, 
             radius=350,
             color= "Blue", 
             stroke = FALSE, 
             fillOpacity = 0.5)%>% 
  setView(lng = -105, lat = 40, zoom = 8)



```




#####  **Most common words (Within Affected Counties)**

A quick view of the most common words in the whole dataset: 

```{r data_subsets_tidy2, echo=FALSE, message=FALSE}

#This is mainly to remove geometry to make computation faster

tweet_in_county_df <- tweet_in_county %>% 
  mutate(tweet = text) %>% # To keep a column with the original tweet
  st_set_geometry(NULL)

```


```{r one-token-per-document-per-row-clusters2, echo=FALSE, message=FALSE}

tweet_in_county_tidy <- tweet_in_county_df %>% 
  unnest_tokens(word, text, token = "tweets") %>% # add a row for each token (word) and repeat the other information
  anti_join(stop_words)  # remove stopwords
  
tweet_in_county_tidy %>% 
  count(word, sort = TRUE) # Count words

```

Again, since **"boulder"** is the most common word and is going to have a big effect in our topic modelling, it was removed from the dataset. The following four terms *("boulderflood", "colorado", "cowx", "flood")* were also excluded because they were so common and used neutrally in all four stages. After excluding these six terms, the new list of common words looks as follows:
          
                                 
```{r remove_words}

keywords <- c("boulder", "boulderflood", "cowx", "$", "amp", "|", "rt", "coflood", "colorado")

tweet_in_county_tidy <- tweet_in_county_df %>% 
  unnest_tokens(word, text, token = "tweets") %>% # add a row for each token (word) and repeat the other information
  anti_join(stop_words) %>%   # remove stopwords
  filter(!word %in% keywords) 

tweet_in_county_tidy %>% 
  count(word, sort = TRUE) # Count words
```


<br>

The statistic **tf-idf** was computed in order to identify which words are important in each of the flood stages. It measures how important a word is to a document in a collection (or corpus) of documents, in our case, it measures how important a word is to a tweet in a collection of tweets. In this case the collection of tweets was the set of tweets belonging to each stage.

```{r explore_tf_idf, fig.width=9, fig.height=8}

tweet_in_county_tf_idf <- tweet_in_county_tidy %>% 
  count(stage, word, sort = TRUE) %>% 
  bind_tf_idf(word, stage, n) %>%
  arrange(-tf_idf) %>%
  group_by(stage) %>% 
  top_n(10) %>% 
  ungroup 

tweet_in_county_tf_idf %>% 
  mutate(word = reorder_within(word, tf_idf, stage)) %>% #reordering by tf_idf
  ggplot(aes(word, tf_idf, fill = stage)) +
  geom_col(alpha = 0.8, show.legend = FALSE) +
  facet_wrap(~stage, scales = "free", ncol = 2)+
  scale_x_reordered() +
  coord_flip() +
  theme(strip.text=element_text(size=11)) +
  labs(x = NULL, y = "tf-idf",
       title = "Highest tf-idf words in tweets during the different stages of Colorado Flood",
       subtitle = "Words importance within tweets sent during each stage")
```

<br>

```{r wordclouds}
library(wordcloud)

colo_tf_idf <- tweet_in_county_tidy %>% 
  count(stage, word, sort = TRUE) %>% 
  bind_tf_idf(word, stage, n) %>%
  arrange(-tf_idf) %>%
  group_by(stage) %>% 
  top_n(100) %>% 
  ungroup 

colo_tf_idf %>% 
  mutate(word = reorder_within(word, tf_idf, stage)) 

stages <- list("preflood", "flood", "immediate_aftermath", "postflood") # To use when I create function

create_wordcloud <- function(stager){
  
  colo_stage <- colo_tf_idf %>%
    filter(stage == stager)
  
  png(filename = paste("Outputs/", stager,'_Wordcloud_Counties.png', sep = ""), width=5, height=5, 
      units="in", res=300) 
  
  set.seed(1234)
  wordcloud(colo_stage$word, colo_stage$n, scale=c(4,0.6),
                  max.words = 20,  # plot the 30 most common words
                  random.order = FALSE, 
                  rot.per = FALSE, 
                  use.r.layout = FALSE,
                  color = brewer.pal(6, "Dark2"),
                  family = "Helvetica")

  dev.off()
}

lapply(stages, create_wordcloud)
```
<br>



#####  **Topic Modeling**


```{r create_matrix, echo=FALSE, message=FALSE}

library(stm) #fast compared to other implementations of topic models. Base don C++
library(quanteda)

# Either one (of the following) works as input for the topic modelling

# Create Document-Term Matrix
tweet_in_county_dfm <- tweet_in_county_tidy %>%
  count(tweet, word, sort = TRUE) %>% 
  cast_dfm(tweet, word, n) # 4 documents (4 stages). Quanteda document frecuency. Special implementation for document term matrix

# Create Sparce Matrix
tweet_in_county_sparse <- tweet_in_county_tidy %>%
  count(tweet_id, word) %>%
  cast_sparse(tweet_id, word, n)

saveRDS(tweet_in_county_sparse, file = "tweet_in_county_sparce.rds")
```




```{r plot_topic_modeling_results, echo=FALSE, message=FALSE, fig.width=10, fig.height=7}

k_result_county <- readRDS(file = "k_result_county.rds")

k_result_county %>%
  transmute(K,
            `Lower bound` = lbound,
            Residuals = map_dbl(residual, "dispersion"),
            `Semantic coherence` = map_dbl(semantic_coherence, mean),
            `Held-out likelihood` = map_dbl(eval_heldout, "expected.heldout")) %>%
  gather(Metric, Value, -K) %>%
  ggplot(aes(K, Value, color = Metric)) +
  geom_line(size = 1.5, alpha = 0.7, show.legend = FALSE) +
  facet_wrap(~Metric, scales = "free_y") +
  labs(x = "K (number of topics)",
       y = NULL,
       title = "Model diagnostics by number of topics",
       subtitle = "These diagnostics indicate that a good number of topics would be around 13")
```


```{r plot_semantic_and_exclusivity, echo=FALSE, message=FALSE, fig.width=9, fig.height=6}

k_result_county %>%
  select(K, exclusivity, semantic_coherence) %>%
  filter(K %in% c(9,11,14,19)) %>% # This values should be the same used when creating the model in the R script
  unnest() %>%
  mutate(K = as.factor(K)) %>%
  ggplot(aes(semantic_coherence, exclusivity, color = K)) +
  geom_point(size = 2, alpha = 0.7) +
  labs(x = "Semantic coherence",
       y = "Exclusivity",
       title = "Comparing exclusivity and semantic coherence",
       subtitle = "Models with fewer topics have higher semantic coherence for more topics, but lower exclusivity")

```


```{r beta, echo=FALSE, message=FALSE, fig.width=13, fig.height=13}
topic_model_county <- k_result_county %>% 
  filter(K == 14) %>% 
  pull(topic_model) %>% 
  .[[1]]

# topic_model

td_beta_county <- tidy(topic_model_county) # The beta matrix shows what are the words that contribute to each topic.

td_beta_county %>% 
  group_by(topic) %>% 
  top_n(10, beta) %>% 
  ungroup %>% 
  mutate(topic = paste0("Topic ", topic),
         term = reorder(term, beta)) %>% #reordering by tf_idf
  ggplot(aes(term, beta, fill = as.factor(topic))) +
  geom_col(alpha = 0.8, show.legend = FALSE) +
  facet_wrap(~topic, scales = "free_y") +
  coord_flip() +
  scale_x_reordered() +
  labs(x = NULL, y = expression(beta),
       title = "Highest word probabilities for each topic",
       subtitle = "Different words are associated with different topics") # To see which words contribute the most to each topic
```



```{r gamma, echo=FALSE, message=FALSE, warning=FALSE}
library(ggthemes)
library(extrafont)

td_gamma_county <- tidy(topic_model_county, matrix = "gamma",                    
                 document_names = rownames(tweet_in_county_sparse))

top_terms_county <- td_beta_county %>%
  arrange(beta) %>%
  group_by(topic) %>%
  top_n(7, beta) %>%
  arrange(-beta) %>%
  select(topic, term) %>%
  summarise(terms = list(term)) %>%
  mutate(terms = map(terms, paste, collapse = ", ")) %>% 
  unnest()

gamma_terms_county <- td_gamma_county %>%
  group_by(topic) %>%
  summarise(gamma = mean(gamma)) %>%
  arrange(desc(gamma)) %>%
  left_join(top_terms_county, by = "topic") %>%
  mutate(topic = paste0("Topic ", topic),
         topic = reorder(topic, gamma))

gamma_terms_county %>%
  top_n(18, gamma) %>%
  ggplot(aes(topic, gamma, label = terms, fill = topic, alpha = 0.7)) +
  geom_col(show.legend = FALSE) +
  geom_text(hjust = 0, nudge_y = 0.0005, size = 3,
            family = "CM Roman") +
  coord_flip() +
  scale_y_continuous(expand = c(0,0),
                     limits = c(0, 0.30),
                     labels = percent_format()) +
  theme_tufte(base_family = "CM Roman",ticks = FALSE) +
  theme(plot.title = element_text(size = 14,
                                  family="CM Roman"),
        plot.subtitle = element_text(size = 11),
        legend.position = "none") +
  labs(x = NULL, y = expression(gamma),
       title = "Topics by prevalence in the dataset containing all Colorado tweets",
       subtitle = "With the top words that contribute to each topic")

```

```{r prepare_heatmap, echo=FALSE, message=FALSE}

heat_county <- td_gamma_county %>% 
  rename(tweetid = document) %>% 
  mutate(tweet_id = as.integer(tweetid)) %>% 
  left_join(colo_clean, by = "tweet_id") %>% 
  select(tweet_id, topic, gamma, day)

# Assigning a topic to each tweet based on which one was had the maximum Gamma
heat_county <- heat_county %>% 
  group_by(tweet_id) %>% 
  filter(gamma == max(gamma)) %>% 
  ungroup()

# Compute the total number of tweets belonging to each topic each day
heat_county <- heat_county %>% 
  group_by(day, topic) %>% 
  add_count() %>% 
  rename(total_topic_day = n) %>% 
  ungroup()

# Compute the total number of tweets per day
heat_county <- heat_county %>% 
  group_by(day) %>% 
  add_count() %>% 
  rename(total_tweets_day = n) %>% 
  mutate(dens = total_topic_day/total_tweets_day)

# Standarazing tweet density (total_topic_day). Computing Z-score values for total_topic_day
scale_this <- function(x){
  (x - mean(x, na.rm=TRUE)) / sd(x, na.rm=TRUE)
}

scaled_heat_county <- 
  heat_county %>%
  group_by(day) %>%
  mutate(scaled_total_topic_day = scale_this(total_topic_day))

```


```{r gammaclusters2_heatplot, echo=FALSE, message=FALSE, fig.width=10, fig.height=6}
#Assign color variables
col1 = "#d8e1cf" 
col2 = "#438484"


library(RColorBrewer)
ggplot(scaled_heat_county, aes(day, as.factor(topic))) +
  geom_tile(aes(fill = scaled_total_topic_day), color = "white") +
  scale_fill_gradient(low = col1, high = col2) +  
  ylab("Topic") +
  xlab("Flood day") +
  theme(legend.title = element_text(size = 10),
        legend.text = element_text(size = 12),
        plot.title = element_text(size=16),
        axis.title=element_text(size=12,face="bold"),
        axis.text.x = element_text(angle = 90, hjust = 1)) +
  labs(fill = "")
```