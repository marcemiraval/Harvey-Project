---
title: "Colorado"
author: "Marcela Su√°rez"
date: "August 10, 2019"
output: html_document
---
<br>


This document presents the results obtained for Colorado.  

<br>

## **Analysis with the whole dataset**

#### **A general  overview of the dataset**

```{r import_data, echo=FALSE, message=FALSE, tidy=TRUE}

library(tidyverse) 
colorado <- read_csv("ColoradoFloodOriginal.csv") # Loading data into R

 # Create a subset with the variables to be used
library(lubridate) # For dates
colorado <- colorado %>% 
  mutate(date = ymd_hms(colorado$created_at, tz='America/Denver')) %>% # Mountain Daylight Time. Time zone in Colorado (GMT-6)
  select(lat = latitude, 
         lon = longitude, 
         date = date, 
         state = c_state,
         text = t_text,
         user = u_name,
         u_profile = u_description)

str(colorado) # shows the structure of the dataframe
```

Tweets were collected from `r min(colorado$date)` to `r max(colorado$date)`  

<br>


####  **Cleaning dataset**

* *Removing retweets:*
```{r remove_retweets, echo=FALSE, message=FALSE}

library(tidytext)
library(stringr)

colorado <- colorado %>% 
  filter(!str_detect(text, "^RT")) %>% 
  filter(!str_detect(text, "#TweetMyJobs")) %>% 
  filter(!str_detect(text, "#GJCO")) %>% 
  filter(!str_detect(text, "#Job")) %>% 
  filter(!str_detect(text, "I'm")) # There will remain some retuits since those will be commented retweets (actually adding some info)
```
    From the original dataset containing 5915 tweets, only 5846 were retained.  

* *Removing tweets sent by bots:*
```{r remove_bots, echo=FALSE, message=FALSE}
colorado <- colorado %>% 
  filter(!str_detect(u_profile, "TwitZip")) %>% 
  filter(!str_detect(u_profile, "TMJ-COB PM Jobs")) %>% 
  filter(!str_detect(u_profile, "JobsDirectUSA.com")) %>% 
  filter(!str_detect(u_profile, "Whole Foods")) %>% 
  filter(!str_detect(u_profile, "Superpages Colorado"))

```
    Tweets sent by [TwitZIP](http://twitzip.com/), a bot service to discover local neighborhood news, were removed. After this process 4916 were retained. Other three bot accounts *("TMJ-COB PM Jobs", "JobsDirectUSA.com", "Whole Foods")* were detected and tweets were removed. A total of 4158 tweets were retained.
    
* *Removing urls, usernames, punctuation and numbers:*
```{r clean_tweets, echo=FALSE, message=FALSE}

colo_clean <- colorado %>% 
   mutate(text = str_remove_all(text, 'http[^ ]+'), #Finally removing urls in a nice way
          text = str_remove_all(text, '@[^ ]+'), # Removing usernames
          text = str_remove_all(text, "[:punct:]"), # Removing puntuation
          text = str_remove_all(text, "[:digit:]"), # Removing numbers
          text = str_remove_all(text, "|")) 
```
    All references to usernames, urls and punctuation and numbers were removed from each tweet in order to prepare the dataset for tokenization.



```{r tweets distribution, echo=FALSE, message=FALSE, warning=FALSE}
library(scales)
gen_hist <- ggplot(colo_clean, aes(x = colo_clean$date)) +
  geom_histogram(aes(y = ..density..), colour = "#878787", fill = "white", binwidth = 3600) +
  geom_density(alpha=.2, colour = "#35978f", fill="#35978f") +# Overlay with transparent density plot
  scale_x_datetime(name = "Date", breaks = date_breaks("2 day"),
                   labels = date_format("%m/%d"), expand = c(0.01,0),
                   limits = c(
                     as.POSIXct(min(colo_clean$date)),
                     as.POSIXct(max(colo_clean$date))
                   )) + scale_y_continuous(name = "Density") +
  ggtitle("")   

gen_hist

```


```{r days_asfactor, echo=FALSE, message=FALSE}

colo_clean <- colo_clean %>% 
  mutate(day = as.factor(day(date))) %>%
  mutate(tweet_id = row_number())


sep15_count <- colo_clean %>% 
  filter(day == 15) %>% 
  nrow()
```

Note: There were only 5 observations on September 15 and only 131 on September 16.

<br>


####  **Temporal stages**

Four stages were defined:

```{r define_temporal_stages, echo=FALSE, message=FALSE}

colo_clean <- colo_clean %>%  # The way to define them in a tidyway
  mutate(stage = ifelse(date <= "2013-09-11 00:00:00 MDT", "preflood", 
                        ifelse(date <= "2013-09-16 00:00:00 MDT", "flood",
                               ifelse(date <= "2013-09-23 00:00:00 MDT", "immediate_aftermath", "postflood")))) %>%
  mutate(stage = factor(stage, levels = unique(stage))) %>% # Turning stages into factors to use them for plotting later
  mutate(stage = factor(stage,levels(stage)[c(3, 4, 2, 1)])) # To plot in an specific order

```

* *Preflood*: From `r min(colorado$date)` to 2013-09-11 00:00:00.
* *Flood*: From 2013-09-11 00:00:00 to 2013-09-16 00:00:00.
* *Immediate Aftermath*: From 2013-09-16 00:00:00 to 2013-09-23 00:00:00.
* *Postflood*: From 2013-09-23 00:00:00 to `r max(colorado$date)`.


<br>



####  **Most common words**

A quick view of the most common words in the whole dataset: 

```{r one-token-per-document-per-row, echo=FALSE, message=FALSE}

colo_tidy <- colo_clean %>% 
  mutate(tweet = text) %>% # To keep a column with the original tweet
  unnest_tokens(word, text, token = "tweets") %>% # add a row for each token (word) and repeat the other information
  anti_join(stop_words) # remove stopwords

colo_tidy %>% 
  count(word, sort = TRUE) # Count words
```

Since **"boulder"** is the most common word and is going ot have a big effect in our topic modelling, it was removed from the dataset. The following five terms *("boulderflood", "colorado", "coflood", "cowx", "flood")* were also excluded because they were so common and used neutrally in all four stages. After excluding these six terms, the new list of common words looks as follows:
                                 
                                 
```{r remove_words, echo=FALSE, message=FALSE}

#keywords <- c("boulder", "boulderflood", "colorado", "coflood", "cowx", "flood", "amp")
keywords <- c("boulder", "amp", "|")

colo_tidy <- colo_clean %>% 
  mutate(tweet = text) %>% # To keep a column with the original tweet
  unnest_tokens(word, text, token = "tweets") %>% # add a row for each token (word) and repeat the other information
  anti_join(stop_words) %>%  # remove stopwords
  filter(!word %in% keywords) # Next two candidates to be removed are Im and boulderflood. TO CHECK LATER!

colo_tidy %>% 
  count(word, sort = TRUE) # Count words
```


```{r save_files, echo=FALSE, message=FALSE}
saveRDS(colo_clean, file = "colo_clean.rds")
saveRDS(colo_tidy, file = "colo_tidy.rds")
```


<br>

The statistic **tf-idf** was computed in order to identify which words are important in each of the flood stages. It measures how important a word is to a document in a collection (or corpus) of documents, in our case, it measures how important a word is to a tweet in a collection of tweets. In this case the collection of tweets was the set of tweets belonging to each stage.

```{r explore_tf_idf, echo=FALSE, message=FALSE}

colo_tf_idf <- colo_tidy %>% 
  count(stage, word, sort = TRUE) %>% 
  bind_tf_idf(word, stage, n) %>%
  arrange(-tf_idf) %>%
  group_by(stage) %>% 
  top_n(10) %>% 
  ungroup 

colo_tf_idf %>% 
  mutate(word = reorder_within(word, tf_idf, stage)) %>% #reordering by tf_idf
  ggplot(aes(word, tf_idf, fill = stage)) +
  geom_col(alpha = 0.8, show.legend = FALSE) +
  facet_wrap(~stage, scales = "free", ncol = 2)+
  scale_x_reordered() +
  coord_flip() +
  theme(strip.text=element_text(size=11)) +
  labs(x = NULL, y = "tf-idf",
       title = "Highest tf-idf words in tweets during the different stages of Colorado Flood",
       subtitle = "Words importance within tweets sent during each stage")


```

<br>



####  **Topic Modeling**

After playing with different numbers, I decided to train a topic model with 4 topics. From 5 on, topics started to look very similar (with the same bag of words). Here is a summary of the results after this process: 

```{r topic_modeling, echo=FALSE, message=FALSE, results="hide"}
library(stm) #fast compared to other implementations of topic models. Base don C++
library(quanteda)

colo_dfm <- colo_tidy %>%
  count(tweet, word, sort = TRUE) %>% 
  cast_dfm(tweet, word, n) # 4 documents (4 stages). Quanteda document frecuency. Special implementation for document term matrix

# Let's train oir topic model

topic_model <- stm(colo_dfm, K = 13, init.type = "Spectral") # unsupervised 
summary(topic_model)
```

The probabilities that each word is generated from each topic are shown below:

```{r beta, echo=FALSE, message=FALSE, fig.width=10, fig.height=8}
td_beta <- tidy(topic_model) # The beta matrix shows what are the words that contribute to each topic.

td_beta %>% 
  group_by(topic) %>% 
  top_n(10, beta) %>% 
  ungroup %>% 
  mutate(topic = paste0("Topic ", topic),
         term = reorder(term, beta)) %>% #reordering by tf_idf
  ggplot(aes(term, beta, fill = as.factor(topic))) +
  geom_col(alpha = 0.8, show.legend = FALSE) +
  facet_wrap(~topic, scales = "free_y") +
  coord_flip() +
  scale_x_reordered() +
  labs(x = NULL, y = expression(beta),
       title = "Highest word probabilities for each topic",
       subtitle = "Different words are associated with different topics") # To see which words contribute the most to each topic
```


Besides estimating each topic as a mixture of words, LDA also models each document (tweet in our case) as a mixture of topics. We can examine the per-document-per-topic probabilities, called the "gamma probability". So, after looking at the probability that each document (tweet) is generated from each topic, we assigned to each document the topic that has the maximum probability.


```{r gamma, echo=FALSE, message=FALSE}
td_gamma <- tidy(topic_model, matrix = "gamma",                    
                 document_names = rownames(colo_dfm))

heat <- td_gamma %>% 
  rename(text = document) %>% 
  left_join(colo_clean, by = "text") %>% 
  select(text, topic, gamma, day)

```

```{r prepare_heatmap, echo=FALSE, message=FALSE}

# Assigning a topic to each tweet based on which one was had the maximum Gamma
heat <- heat %>% 
  group_by(text) %>% 
  filter(gamma == max(gamma)) %>% 
  ungroup()

# Compute the total number of tweets belonging to each topic each day
heat <- heat %>% 
  group_by(day, topic) %>% 
  add_count() %>% 
  rename(total_topic_day = n) %>% 
  ungroup()

# Compute the total number of tweets per day
heat <- heat %>% 
  group_by(day) %>% 
  add_count() %>% 
  rename(total_tweets_day = n) %>% 
  mutate(dens = total_topic_day/total_tweets_day)

# Standarazing tweet density (total_topic_day). Computing Z-score values for total_topic_day
scale_this <- function(x){
  (x - mean(x, na.rm=TRUE)) / sd(x, na.rm=TRUE)
}

scaled_heat <- 
  heat %>%
  group_by(day) %>%
  mutate(scaled_total_topic_day = scale_this(total_topic_day))

```


```{r gammaclusters2_heatplot, echo=FALSE, message=FALSE}
#Assign color variables
col1 = "#d8e1cf" 
col2 = "#438484"


library(RColorBrewer)
ggplot(scaled_heat, aes(day, as.factor(topic))) +
  geom_tile(aes(fill = scaled_total_topic_day), color = "white") +
  scale_fill_gradient(low = col1, high = col2) +  
  ylab("Topic") +
  xlab("Flood day") +
  theme(legend.title = element_text(size = 10),
        legend.text = element_text(size = 12),
        plot.title = element_text(size=16),
        axis.title=element_text(size=12,face="bold"),
        axis.text.x = element_text(angle = 90, hjust = 1)) +
  labs(fill = "")
```

From this, we can't demonstrate yet a clear association of these topics with the four stages of the disaster.


