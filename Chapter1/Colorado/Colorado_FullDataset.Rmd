---
title: "Colorado"
author: "Marcela Su√°rez"
date: "June 17, 2020"
output: html_document
editor_options: 
  chunk_output_type: inline
---
<br>


This document presents the results obtained for Colorado.  

<br>

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo=FALSE, message=FALSE, tidy=TRUE, warning=FALSE)
```

## **Analysis with the whole dataset**

#### **A general  overview of the dataset**

```{r import_data}
library(tidyverse) 
 # setwd(dir= "Chapter1/Colorado/")
colorado <- read_csv("Data/ColoradoFloodOriginal.csv") # Loading data into R
 # Create a subset with the variables to be used
library(lubridate) # For dates
colorado <- colorado %>% 
  mutate(date = ymd_hms(colorado$created_at, tz='America/Denver')) %>% # Mountain Daylight Time. Time zone in Colorado (GMT-6)
  select(lat = latitude, 
         lon = longitude, 
         date = date, 
         state = c_state,
         text = t_text,
         user = u_name,
         u_profile = u_description)
str(colorado) # shows the structure of the dataframe
```

Tweets were collected from `r min(colorado$date)` to `r max(colorado$date)`  

<br>


####  **Cleaning dataset**

* *Removing retweets:*
```{r remove_retweets}
library(tidytext)
library(stringr)
colorado <- colorado %>% 
  filter(!str_detect(text, "^RT")) %>% 
  filter(!str_detect(text, "#TweetMyJobs")) %>% 
  filter(!str_detect(text, "#GJCO")) %>% 
  filter(!str_detect(text, "#Job")) %>% 
  filter(!str_detect(text, "I'm")) # There will remain some retuits since those will be commented retweets (actually adding some info)
```
    From the original dataset containing 5915 tweets, only 4837 were retained.  

* *Removing tweets sent by bots:*
```{r remove_bots}
colorado <- colorado %>% 
  filter(!str_detect(u_profile, "TwitZip")) %>% 
  filter(!str_detect(u_profile, "TMJ-COB PM Jobs")) %>% 
  filter(!str_detect(u_profile, "JobsDirectUSA.com")) %>% 
  filter(!str_detect(u_profile, "Whole Foods")) %>% 
  filter(!str_detect(u_profile, "Superpages Colorado"))
```
    Tweets sent by [TwitZIP](http://twitzip.com/), a bot service to discover local neighborhood news, were removed. After this process 4086 were retained. Other three bot accounts *("TMJ-COB PM Jobs", "JobsDirectUSA.com", "Whole Foods")* were detected and tweets were removed. A total of 4086 tweets were retained.
    
* *Removing urls, usernames, punctuation and numbers:*
```{r clean_tweets}
colo_clean <- colorado %>% 
   mutate(text = str_remove_all(text, 'http[^ ]+'), #Finally removing urls in a nice way
          text = str_remove_all(text, '@[^ ]+'), # Removing usernames
          text = str_remove_all(text, "[:punct:]"), # Removing puntuation
          text = str_remove_all(text, "[:digit:]"), # Removing numbers
          text = str_remove_all(text, "|")) 
```
    All references to usernames, urls and punctuation and numbers were removed from each tweet in order to prepare the dataset for tokenization.



```{r tweets_distribution}
library(scales)
gen_hist <- ggplot(colo_clean, aes(x = colo_clean$date)) +
  geom_histogram(aes(y = ..density..), colour = "#878787", fill = "white", binwidth = 3600) +
  geom_density(alpha=.2, colour = "#35978f", fill="#35978f") +# Overlay with transparent density plot
  scale_x_datetime(name = "Date", breaks = date_breaks("2 day"),
                   labels = date_format("%m/%d"), expand = c(0.01,0),
                   limits = c(
                     as.POSIXct(min(colo_clean$date)),
                     as.POSIXct(max(colo_clean$date))
                   )) + scale_y_continuous(name = "Density") +
  ggtitle("")   
gen_hist
```


```{r days_asfactor}
colo_clean <- colo_clean %>% 
  mutate(day = as.factor(day(date))) %>%
  mutate(tweet_id = row_number())
sep15_count <- colo_clean %>% 
  filter(day == 15) %>% 
  nrow()
```

Note: There were only 5 observations on September 15 and only 131 on September 16.

In this stage almost **6%** of the tweets were excluded. 228 tweets from the 4086 total tweets.

```{r remove_topuser}
user_frequency <- colo_clean %>%
        group_by(user) %>%
        count()
colo_clean <- colo_clean %>% 
  filter(user != "Denver CP")
```



<br>


####  **Temporal stages**

Four stages were defined:

```{r define_temporal_stages}
colo_clean <- colo_clean %>%  # The way to define them in a tidyway
  mutate(stage = ifelse(date <= "2013-09-11 00:00:00 MDT", "preflood", 
                        ifelse(date <= "2013-09-16 00:00:00 MDT", "flood",
                               ifelse(date <= "2013-09-23 00:00:00 MDT", "immediate_aftermath", "postflood")))) %>%
  mutate(stage = factor(stage, levels = unique(stage))) %>% # Turning stages into factors to use them for plotting later
  mutate(stage = factor(stage,levels(stage)[c(3, 4, 2, 1)])) # To plot in an specific order
```

* *Preflood*: From `r min(colorado$date)` to 2013-09-11 00:00:00.
* *Flood*: From 2013-09-11 00:00:00 to 2013-09-16 00:00:00.
* *Immediate Aftermath*: From 2013-09-16 00:00:00 to 2013-09-23 00:00:00.
* *Postflood*: From 2013-09-23 00:00:00 to `r max(colorado$date)`.

Total number of tweets by stage:

```{r tweets_by_stage}
frequency_by_stage <- colo_clean %>%
        group_by(stage) %>%
        count()
knitr::kable(frequency_by_stage, format="html")
```


<br>



####  **Most common words**

A quick view of the most common words in the whole dataset: 

```{r one-token-per-document-per-row}
colo_tidy <- colo_clean %>% 
  mutate(tweet = text) %>% # To keep a column with the original tweet
  unnest_tokens(word, text, token = "tweets") %>% # add a row for each token (word) and repeat the other information
  anti_join(stop_words) # remove stopwords
colo_tidy %>% 
  count(word, sort = TRUE) # Count words
```

Since **"boulder"** is the most common word and is going ot have a big effect in our topic modelling, it was removed from the dataset. After this process the new list of common words looks as follows:
                                 
                                 
```{r remove_words}
keywords <- c("boulder", "boulderflood", "cowx", "$", "amp", "|", "rt", "coflood", "colorado", "flood")
colo_tidy <- colo_clean %>% 
  mutate(tweet = text) %>% # To keep a column with the original tweet
  unnest_tokens(word, text, token = "tweets") %>% # add a row for each token (word) and repeat the other information
  anti_join(stop_words) %>%  # remove stopwords
  filter(!word %in% keywords) # Next two candidates to be removed are Im and boulderflood. TO CHECK LATER!
colo_tidy %>% 
  count(word, sort = TRUE) # Count words
```


```{r save_files}
saveRDS(colo_clean, file = "colo_clean.rds")
saveRDS(colo_tidy, file = "colo_tidy.rds")
```


<br>

The statistic **tf-idf** was computed in order to identify which words are important in each of the flood stages. It measures how important a word is to a document in a collection (or corpus) of documents, in our case, it measures how important a word is to a tweet in a collection of tweets. In this case the collection of tweets was the set of tweets belonging to each stage.

```{r explore_tf_idf, fig.width=9, fig.height=8}
colo_tf_idf <- colo_tidy %>% 
  count(stage, word, sort = TRUE) %>% 
  bind_tf_idf(word, stage, n) %>%
  arrange(-tf_idf) %>%
  group_by(stage) %>% 
  top_n(10) %>% 
  ungroup 
colo_tf_idf %>% 
  mutate(word = reorder_within(word, tf_idf, stage)) %>% #reordering by tf_idf
  ggplot(aes(word, tf_idf, fill = stage)) +
  geom_col(alpha = 0.8, show.legend = FALSE) +
  facet_wrap(~stage, scales = "free", ncol = 2)+
  scale_x_reordered() +
  coord_flip() +
  theme(strip.text=element_text(size=11)) +
  labs(x = NULL, y = "tf-idf",
       title = "Highest tf-idf words in tweets during the different stages of Colorado Flood",
       subtitle = "Words importance within tweets sent during each stage")
```

<br>

```{r wordclouds, eval=FALSE}
library(wordcloud)
colo_tf_idf <- colo_tidy %>% 
  count(stage, word, sort = TRUE) %>% 
  bind_tf_idf(word, stage, n) %>%
  arrange(-tf_idf) %>%
  group_by(stage) %>% 
  top_n(100) %>% 
  ungroup 
colo_tf_idf %>% 
  mutate(word = reorder_within(word, tf_idf, stage)) 
stages <- list("preflood", "flood", "immediate_aftermath", "postflood") # To use when I create function
create_wordcloud <- function(stager){
  
  colo_stage <- colo_tf_idf %>%
    filter(stage == stager)
  
  png(filename = paste("Outputs/", stager,'_Wordcloud_FullDataset.png', sep = ""), width=5, height=5, 
      units="in", res=300) 
  
  set.seed(1234)
  wordcloud(colo_stage$word, colo_stage$n, scale=c(4,0.6),
                  max.words = 20,  # plot the 30 most common words
                  random.order = FALSE, 
                  rot.per = FALSE, 
                  use.r.layout = FALSE,
                  color = brewer.pal(6, "Dark2"))
  dev.off()
}
lapply(stages, create_wordcloud)
```


####  **Topic Modeling**


```{r create_matrix}
library(stm) #fast compared to other implementations of topic models. Base don C++
library(quanteda)
# Either one (of the following) works as input for the topic modelling
# Create Document-Term Matrix
colo_dfm <- colo_tidy %>%
  count(tweet, word, sort = TRUE) %>% 
  cast_dfm(tweet, word, n) # 4 documents (4 stages). Quanteda document frecuency. Special implementation for document term matrix
# Create Sparce Matrix
colo_sparse <- colo_tidy %>%
  count(tweet_id, word) %>%
  cast_sparse(tweet_id, word, n)
```


```{r plot_topic_modeling_results, fig.width=10, fig.height=7}
k_result <- readRDS(file = "Data/k_result.rds")
k_result %>%
  transmute(K,
            `Lower bound` = lbound,
            Residuals = map_dbl(residual, "dispersion"),
            `Semantic coherence` = map_dbl(semantic_coherence, mean),
            `Held-out likelihood` = map_dbl(eval_heldout, "expected.heldout")) %>%
  gather(Metric, Value, -K) %>%
  ggplot(aes(K, Value, color = Metric)) +
  geom_line(size = 1.5, alpha = 0.7, show.legend = FALSE) +
  facet_wrap(~Metric, scales = "free_y") +
  labs(x = "K (number of topics)",
       y = NULL,
       title = "Model diagnostics by number of topics",
       subtitle = "These diagnostics indicate that a good number of topics would be around 13")
```

```{r plot_semantic_and_exclusivity, fig.width=9, fig.height=6}
k_result %>%
  select(K, exclusivity, semantic_coherence) %>%
  filter(K %in% c(9,13,28)) %>% # This values should be the same used when creating the model in the R script
  unnest() %>%
  mutate(K = as.factor(K)) %>%
  ggplot(aes(semantic_coherence, exclusivity, color = K)) +
  geom_point(size = 2, alpha = 0.7) +
  labs(x = "Semantic coherence",
       y = "Exclusivity",
       title = "Comparing exclusivity and semantic coherence",
       subtitle = "Models with fewer topics have higher semantic coherence for more topics, but lower exclusivity")
```

```{r beta, fig.width=10, fig.height=8}
topic_model <- k_result %>% 
  filter(K == 13) %>% 
  pull(topic_model) %>% 
  .[[1]]
# topic_model
td_beta <- tidy(topic_model) # The beta matrix shows what are the words that contribute to each topic.
td_beta %>% 
  group_by(topic) %>% 
  top_n(10, beta) %>% 
  ungroup %>% 
  mutate(topic = paste0("Topic ", topic),
         term = reorder(term, beta)) %>% #reordering by tf_idf
  ggplot(aes(term, beta, fill = as.factor(topic))) +
  geom_col(alpha = 0.8, show.legend = FALSE) +
  facet_wrap(~topic, scales = "free_y") +
  coord_flip() +
  scale_x_reordered() +
  labs(x = NULL, y = expression(beta),
       title = "Highest word probabilities for each topic",
       subtitle = "Different words are associated with different topics") # To see which words contribute the most to each topic
```


```{r gamma}
library(ggthemes)
library(extrafont)
td_gamma <- tidy(topic_model, matrix = "gamma",                    
                 document_names = rownames(colo_sparse))
top_terms <- td_beta %>%
  arrange(beta) %>%
  group_by(topic) %>%
  top_n(7, beta) %>%
  arrange(-beta) %>%
  select(topic, term) %>%
  summarise(terms = list(term)) %>%
  mutate(terms = map(terms, paste, collapse = ", ")) %>% 
  unnest()
gamma_terms <- td_gamma %>%
  group_by(topic) %>%
  summarise(gamma = mean(gamma)) %>%
  arrange(desc(gamma)) %>%
  left_join(top_terms, by = "topic") %>%
  mutate(topic = paste0("Topic ", topic),
         topic = reorder(topic, gamma))
gamma_terms %>%
  top_n(17, gamma) %>%
  ggplot(aes(topic, gamma, label = terms, fill = topic, alpha = 0.7)) +
  geom_col(show.legend = FALSE) +
  geom_text(hjust = 0, nudge_y = 0.0005, size = 3,
            family = "CM Roman") +
  coord_flip() +
  scale_y_continuous(expand = c(0,0),
                     limits = c(0, 0.30),
                     labels = percent_format()) +
  theme_tufte(base_family = "CM Roman",ticks = FALSE) +
  theme(plot.title = element_text(size = 14,
                                  family="CM Roman"),
        plot.subtitle = element_text(size = 11),
        legend.position = "none") +
  labs(x = NULL, y = expression(gamma),
       title = "Topics by prevalence in the dataset containing all Colorado tweets",
       subtitle = "With the top words that contribute to each topic")
```

```{r prepare_heatmap}
heat <- td_gamma %>% 
  rename(tweetid = document) %>% 
  mutate(tweet_id = as.integer(tweetid)) %>% 
  left_join(colo_clean, by = "tweet_id") %>% 
  select(tweet_id, topic, gamma, day)
# Assigning a topic to each tweet based on which one was had the maximum Gamma
heat <- heat %>% 
  group_by(tweet_id) %>% 
  filter(gamma == max(gamma)) %>% 
  ungroup()
# Compute the total number of tweets belonging to each topic each day
heat <- heat %>% 
  group_by(day, topic) %>% 
  add_count() %>% 
  rename(total_topic_day = n) %>% 
  ungroup()
# Compute the total number of tweets per day
heat <- heat %>% 
  group_by(day) %>% 
  add_count() %>% 
  rename(total_tweets_day = n) %>% 
  mutate(dens = total_topic_day/total_tweets_day)
# Standarazing tweet density (total_topic_day). Computing Z-score values for total_topic_day
scale_this <- function(x){
  (x - mean(x, na.rm=TRUE)) / sd(x, na.rm=TRUE)
}
scaled_heat <- 
  heat %>%
  group_by(day) %>%
  mutate(scaled_total_topic_day = scale_this(total_topic_day))
```

```{r shapefile}
library(sf)
tweet_and_topic <- dplyr::left_join(heat, colo_clean, by = "tweet_id") %>% 
  select(tweet_id = tweet_id,
         topic = topic,
         text, text,
         user = user,
         u_profile = u_profile,
         date = date,
         lat = lat,
         lon = lon,
         day = day.x,
         state = state) %>% 
  mutate(topic = paste0("Topic ", topic))
```

```{r shapefile}
tweet_and_topic_sf <- tweet_and_topic %>% 
  st_as_sf(coords = c("lon", "lat"), crs = 4326)
#tweet_and_topic$topic <- factor(tweet_and_topic$topic)
#write.csv(tweet_and_topic, file = "tweet_and_topic.csv", row.names = FALSE)
#st_write(tweet_and_topic_sf, "Data/tweet_and_topic.geojson")
```



```{r gammaclusters2_heatplot}
#Assign color variables
col1 = "#d8e1cf" 
col2 = "#438484"
library(RColorBrewer)
ggplot(scaled_heat, aes(day, as.factor(topic))) +
  geom_tile(aes(fill = scaled_total_topic_day), color = "white") +
  scale_fill_gradient(low = col1, high = col2) +  
  ylab("Topic") +
  xlab("Flood day") +
  theme(legend.title = element_text(size = 10),
        legend.text = element_text(size = 12),
        plot.title = element_text(size=16),
        axis.title=element_text(size=12,face="bold"),
        axis.text.x = element_text(angle = 90, hjust = 1)) +
  labs(fill = "")
```


```{r joinTweetsTopics, echo=FALSE, message=FALSE}

tweet_and_topic <- dplyr::left_join(heat, colo_clean, by = "tweet_id") %>% 
  select(tweet_id = tweet_id,
         topic = topic,
         text, text,
         user = user,
         u_profile = u_profile,
         date = date,
         lat = lat,
         lon = lon,
         day = day.x,
         state = state) %>% 
  mutate(topic = paste0("Topic ", topic))

tweet_and_topic$topic <- factor(tweet_and_topic$topic)

```

```{r geoconversion, echo=FALSE, message=FALSE}
tweet_and_topic_geo <- tweet_and_topic %>% 
  st_as_sf(coords = c("lon", "lat"), crs = 4326) 
```


```{r mapping_damage_and_tweets, echo=FALSE, message=FALSE}

library(tmap)

tweet_and_topic_geo1 <- tweet_and_topic_geo %>% 
  filter(topic == "Topic 1" | topic == "Topic 10" | topic == "Topic 2" | topic == "Topic 3")

damagePols <- st_read("Data/GroundTruthData/StructuresPols/DamagePolsAllIncluded.geojson")
tmap_mode("view") 
map <- tm_shape(damagePols) + 
  tm_polygons(col = "#756bb1",
              alpha = 0.8) +
  tm_shape(tweet_and_topic_geo1) + 
  tm_dots(col = "topic",
             scale=1,
             alpha = 1) + 
  tm_basemap(server = "Stamen.TonerLite")

map

```


```{r computing_distance, echo=FALSE, message=FALSE}

# Check if there are points within the polygon. St_within also works for this
pnts <- tweet_and_topic_geo %>% 
  mutate(intersection = as.integer(st_intersects(geometry, damagePols))) %>% 
  filter(state == "Colorado")

# Compute distances from points to the closest polygon
dist <- geosphere::dist2Line(p = st_coordinates(pnts), line = st_coordinates(damagePols)[,1:2])

# bind results with original points
pts.wit.dist <- cbind(pnts, dist) %>% 
  mutate(distancee = ifelse(is.na(intersection), distance, 0)) %>% 
  select(-c(intersection, distance))


```


```{r plotting_distance, echo=FALSE, message=FALSE, fig.width=15, fig.height=10}

pts.wit.dist %>% 
  ggplot(aes(distancee, date, color = topic)) + # 
  geom_point(size = 2, alpha = 0.7) +
  labs(x = "Distance",
       y = "Date",
       title = "Distance between tweets and nearest reported structure",
       subtitle = "Distance to the nearest reported structure and tweet's date") +
  facet_wrap(~topic) +
  theme(legend.position="bottom")

```


```{r distance_histo, echo=FALSE, message=FALSE, fig.width=15}
ggplot(pts.wit.dist, aes(x = distancee)) +
  geom_density(alpha=.2, colour = "#35978f", fill="#35978f")
```



```{r b,fig.width=14, fig.height=10}

library(viridis)
library(hrbrthemes)

pts.wit.dist2 <- pts.wit.dist %>% 
  filter(distancee <= 7000) 

pts.wit.dist2 %>% 
  ggplot(aes(distancee, color = topic)) + 
  geom_freqpoly(binwidth = 500)+
  xlab("Distance") +
  ylab("Number of tweets by distance band") +
  scale_x_continuous(breaks = seq(0, 6000, 1000), expand = c(0, 0), limits = c(0,NA)) +
  scale_color_viridis(discrete=TRUE, guide=FALSE) +
  theme(legend.title = element_blank(),
        legend.background = element_rect(size = 2),
        legend.text = element_text(size = 10),
        plot.background = element_rect(colour = NA),
        axis.title = element_text(size = 12, face = "bold",
                                  margin=margin(30,20,0,0)),
        axis.line = element_line(colour = "Black"),
        axis.text = element_text(size = 11, color = "black"),
        strip.text = element_text(size = 11),
        legend.key.width = unit(1.5, "cm")) +
  facet_wrap(~topic) +
  geom_freqpoly(data=transform(pts.wit.dist2, topic=NULL),
                aes(y = ..count.. / n_distinct(pts.wit.dist$topic)), 
                binwidth = 500, 
                color = "red",
                alpha = 0.5,
                linetype = "longdash")

# The solution to keep totals in facets was given here:
# https://stackoverflow.com/questions/15834897/ggplot2-is-there-a-way-to-overlay-a-single-plot-to-all-facets-in-a-ggplot

```

```{r c,fig.width=14, fig.height=10}

library(viridis)
library(hrbrthemes)

pts.wit.dist %>% 
  filter(distancee <= 7000) %>% 
  ggplot(aes(x = distancee, y = ..count..)) +
  geom_freqpoly(aes( color = topic), 
                binwidth = 500) +
  geom_freqpoly(aes(y = ..count.. / n_distinct(pts.wit.dist$topic)), 
                binwidth = 500, 
                color = "red",
                linetype = "longdash") +
  xlab("Distance") +
  ylab("Number of tweets by distance band") +
  scale_x_continuous(breaks = seq(0, 6000, 1000), expand = c(0, 0), limits = c(0,NA)) +
  scale_color_viridis(discrete=TRUE) + # add in the parenthesis , guide=FALSE if I don't want legend
  theme(legend.title = element_blank(),
        legend.background = element_rect(size = 2),
        legend.text = element_text(size = 10),
        plot.background = element_rect(colour = NA),
        axis.title = element_text(size = 12, face = "bold",
                                  margin=margin(30,20,0,0)),
        axis.line = element_line(colour = "Black"),
        axis.text = element_text(size = 11, color = "black"),
        strip.text = element_text(size = 11),
        legend.key.width = unit(1.5, "cm"))

```




```{r}
library(wordcloud)
library(reshape2)

ap_top_terms <- td_beta %>%
  group_by(topic) %>%
  top_n(200, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)

library(wordcloud)
library(reshape2)

ap_top_terms %>%
  acast(term ~ topic, value.var = "beta", fill = 0) %>%
  comparison.cloud(max.words = 100)


  
# %>% 
#   anti_join(stop_words) %>%
#   count(term) %>%
#   with(wordcloud(term, n, max.words = 100))



 # wordcloud(words = term, freq = beta)

# td_beta %>% 
#   group_by(topic) %>% 
#   top_n(10, beta) %>% 
#   ungroup %>% 
#   mutate(topic = paste0("Topic ", topic),
#          term = reorder(term, beta)) %>% #reordering by tf_idf
#   ggplot(aes(term, beta, fill = as.factor(topic))) +
#   geom_col(alpha = 0.8, show.legend = FALSE) +
#   facet_wrap(~topic, scales = "free_y") +
#   coord_flip() +
#   scale_x_reordered() +
#   labs(x = NULL, y = expression(beta),
#        title = "Highest word probabilities for each topic",
#        subtitle = "Different words are associated with different topics") # To see which words contribute the most to each topic

# Code taken from here: https://stackoverflow.com/questions/45563098/topic-modelling-lda-word-frequency-in-each-topic-and-wordcloud/45577480#45577480

```

