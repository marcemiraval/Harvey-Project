---
title: "Colorado Clusters During Flood and Intermediate Aftermath"
author: "Marcela"
date: "18/05/2020"
output:
  html_document: default
  pdf_document: default
editor_options:
  chunk_output_type: inline
---


<br>

<br>

####  **Data Filtering**

I used the clean dataset produced in the "Colorado_Fulldataset" document in order to analyze only general public response in Twitter, since this dataset already exluded tweets sent by official agencies and bots. The total number of tweets in the dataset is **3858**.


```{r import_data, echo=FALSE, message=FALSE}
# setwd(dir= "Chapter1/Colorado/") # Run in console
library(tidyverse)
colo_clean <- readRDS(file = "Data/colo_clean.rds")
# write_csv(colo_clean, path = "Data/colo_clean.csv")
```

```{r check_top_users, echo=FALSE, message=FALSE}
user_frequency <- colo_clean %>%
        group_by(user) %>%
        count()
```

Tweets sent during the Flood and Inmediate Aftermath phases of the disaster were filtered. This means that **45%** of the tweets were excluded. This is we will use **2132 tweets** from the original 3858 total.

```{r filter_OfficialTweets, echo=FALSE, message=FALSE}
colo_peak_CO <- colo_clean %>% 
  filter(stage == "flood" | stage == "immediate_aftermath") 
# write_csv(colo_peak_CO, path = "Data/colo_Flood_IA.csv")
```

Before any spatial analysis or plotting, the data was first projected in North America Lambert Conformal Conic.

```{r getting_spatial, echo=FALSE, message=FALSE}

library(sf)

# Store tweets as simple features and project data
colo_sf_p <- colo_peak_CO %>% 
  st_as_sf(coords = c("lon", "lat"), crs = 4326)


colo_sf <- colo_sf_p %>% # set WGS84 as original datum
  st_transform(crs ="+proj=lcc +lat_1=20 + lat_2=60 + lat_0=40 +
               lon_0=-96 +x_0=0 +y_0=0 +ellps=GRS80 +datum=NAD83 +
               units=m no_defs") # Projected in North_America_Lambert_Conformal_Conic

st_crs(colo_sf) # Retrieve current coord ref system: EPSG: 4326 WGS84

```


<br>

####  **Subsetting by Spatial Clustering**

One of the goals of this study is to confirm if spatially clustered tweets can serve as a proxy for reports from affected areas. So I will repeat the same analysis done for the whole dataset but now considering only tweets belonging to clusters after the spatial clustering process. A hierarchical implementation of dbscan **(hdbscan)** was used for the spatial clustering. For hdbscan we need to pick a number of minimum points to be considered to identify the cluster. When setting that number with any value between 159 and 225, clusters in Colorado were identify. So we picked the minimum value in that range which retains the maximum number of tweets: **160**.

```{r list_of_minPts, echo=FALSE, message=FALSE}
minPts <- function(a) {
  cuantox <- a*nrow(colo_peak_CO)/100
  return(cuantox)
}

lista <- seq(1, 10, by = 1) 

minPts_list <- lapply(lista, minPts)
```


```{r spatial_clusters, echo=FALSE, message=FALSE}

library(dbscan)
library(leaflet)
library(htmltools)

set.seed(123)

clusters <- hdbscan(colo_sf %>%
                      st_coordinates(), #This rounds coordinates
                    minPts = 161)

colo_clusters <- colo_sf %>% 
  mutate(cluster = clusters$cluster)

## Plotting spatial cluster results

colo_clusters <- colo_clusters %>% # Need to reproject in WGS84 datum. long lat format.
  st_transform(crs = 4326)

colo_clusters$cluster <- as.factor(colo_clusters$cluster) #Clusters as factors for coloring
# pal <- colorFactor(topo.colors(10), clusters$cluster) # In case needed when using leaflet map
```

```{r import_ref_data, echo=FALSE, message=FALSE}
affected_counties <- readRDS(file = "Data/affected_counties.rds")
affected_counties_p <- st_transform(affected_counties, 4326)
```


```{r map_clusters, echo=FALSE, message=FALSE}
library(tmap)

tmap_mode("view")

map <- tm_shape(affected_counties_p) + 
  tm_polygons(col = "grey",
              alpha = 0.8) +
  tm_shape(colo_clusters) + 
  tm_dots(col = "cluster",
          alpha = 1)

map + tm_basemap(server = "Stamen.TonerLite")

```


#####  **Most common words (Within Spatial Clusters)**

After the spatial filtering only tweets sent during the Flood stage only **30%** of the total tweets were retained. **1172 tweets** from the 3858 total.

```{r data_subsets_tidy, echo=FALSE, message=FALSE}

#This is mainly to remove geometry to make computation faster

clusters_df <- colo_clusters %>% 
  filter(cluster == 1 | cluster == 2) %>% 
  mutate(tweet = text) %>% # To keep a column with the original tweet
  st_set_geometry(NULL)

```

A quick view of the most common words in the whole dataset: 

```{r one-token-per-document-per-row-clusters, echo=FALSE, message=FALSE}
library(tidytext)

clusters_tidy <- clusters_df %>% 
  unnest_tokens(word, text, token = "tweets") %>% # add a row for each token (word) and repeat the other information
  anti_join(stop_words)  # remove stopwords
  
clusters_tidy %>% 
  count(word, sort = TRUE) # Count words

```


Again, since **"boulder"** is the most common word and is going to have a big effect in our topic modelling, it was removed from the dataset. The term **"boulderflood")** was also excluded because it was so common and used neutrally in all four stages. After excluding the two terms, the new list of common words looks as follows:
                                 
                                 
```{r remove_words_clusters, echo=FALSE, message=FALSE}

keywords <- c("boulder", "boulderflood") #, "cowx", "$", "amp", "|", "rt", "coflood" Maybe?


clusters_tidy <- clusters_df %>% 
  unnest_tokens(word, text, token = "tweets") %>% # add a row for each token (word) and repeat the other information
  anti_join(stop_words) %>%   # remove stopwords
  filter(!word %in% keywords) 

clusters_tidy %>% 
  count(word, sort = TRUE) # Count words

```



```{r explore_tf_idf_clusters, echo=FALSE, message=FALSE, fig.width=10, fig.height=50}
clusters_tf_idf <- clusters_tidy %>% 
  count(day, word, sort = TRUE) %>% 
  bind_tf_idf(word, day, n) %>%
  arrange(-tf_idf) %>%
  group_by(day) %>% 
  top_n(9) %>% 
  ungroup 

```

```{r explore_tf_idf_clusters2, echo=FALSE, message=FALSE, fig.width=10, fig.height=50}

clusters_tf_idf %>% 
  mutate(word = reorder_within(word, tf_idf, day)) %>% #reordering by tf_idf
  ggplot(aes(word, tf_idf, fill = day)) +
  geom_col(alpha = 0.8, show.legend = FALSE) +
  facet_wrap(~day, scales = "free", ncol = 2)+
  scale_x_reordered() +
  coord_flip() +
  theme(strip.text=element_text(size=11)) +
  labs(x = NULL, y = "tf-idf",
       title = "Highest tf-idf words in tweets during the different stages of Colorado Flood",
       subtitle = "Words importance within tweets sent during each stage")

```


<br>



#####  **Topic Modeling**

Again, after playing with different numbers, I decided to train a topic model with 15 topics. From 16 on, topics started to look very similar (with the same bag of words). Here is a summary of the results after this process: 

```{r topic_modeling_clusters, echo=FALSE, message=FALSE}

library(stm) #fast compared to other implementations of topic models. Base don C++
library(quanteda)


# Filtering only columns needed
clusters_super_tidy <- clusters_tidy %>% 
  select(day,
         tweet_id,
         tweet,
         word)

# Either one (of the following) works as input for the topic modelling

# Create Document-Term Matrix
clusters_floodAndIA_dfm <- clusters_super_tidy %>%
  count(tweet, word, sort = TRUE) %>% 
  cast_dfm(tweet, word, n) # 4 documents (4 stages). Quanteda document frecuency. Special implementation for document term matrix

# Create Sparce Matrix
clusters_floodAndIA_sparse <- clusters_super_tidy %>%
  count(tweet_id, word) %>%
  cast_sparse(tweet_id, word, n)

# saveRDS(clusters_floodAndIA_sparse, file = "Data/clusters_floodAndIA_sparse.rds")
# saveRDS(clusters_floodAndIA_dfm, file = "Data/clusters_floodAndIA_dfm.rds")

```


```{r plot_topic_modeling_results, echo=FALSE, message=FALSE, fig.width=10, fig.height=7}

k_result_floodAndIA_cluster <- readRDS(file = "Data/k_result_floodAndIA_clusters.rds") # This file is in the UCSB box folder since it's too heavy for github

k_result_floodAndIA_cluster %>%
  transmute(K,
            `Lower bound` = lbound,
            Residuals = map_dbl(residual, "dispersion"),
            `Semantic coherence` = map_dbl(semantic_coherence, mean),
            `Held-out likelihood` = map_dbl(eval_heldout, "expected.heldout")) %>%
  gather(Metric, Value, -K) %>%
  ggplot(aes(K, Value, color = Metric)) +
  geom_line(size = 1.5, alpha = 0.7, show.legend = FALSE) +
  facet_wrap(~Metric, scales = "free_y") +
  labs(x = "K (number of topics)",
       y = NULL,
       title = "Model diagnostics by number of topics",
       subtitle = "These diagnostics indicate that a good number of topics would be around 13")
```

```{r plot_semantic_and_exclusivity, echo=FALSE, message=FALSE, fig.width=9, fig.height=6}

k_result_floodAndIA_cluster %>%
  select(K, exclusivity, semantic_coherence) %>%
  filter(K %in% c(6, 8, 10, 14, 22)) %>% # This values should be the same used when creating the model in the R script
  unnest() %>%
  mutate(K = as.factor(K)) %>%
  ggplot(aes(semantic_coherence, exclusivity, color = K)) +
  geom_point(size = 2, alpha = 0.7) +
  labs(x = "Semantic coherence",
       y = "Exclusivity",
       title = "Comparing exclusivity and semantic coherence",
       subtitle = "Models with fewer topics have higher semantic coherence for more topics, but lower exclusivity")

```

```{r beta, echo=FALSE, message=FALSE, fig.width=10, fig.height=8}
topic_model <- k_result_floodAndIA_cluster %>% 
  filter(K == 14) %>% 
  pull(topic_model) %>% 
  .[[1]]

# topic_model

td_beta_clusters <- tidy(topic_model) # The beta matrix shows what are the words that contribute to each topic.

td_beta_clusters %>% 
  group_by(topic) %>% 
  top_n(10, beta) %>% 
  ungroup %>% 
  mutate(topic = paste0("Topic ", topic),
         term = reorder(term, beta)) %>% #reordering by tf_idf
  ggplot(aes(term, beta, fill = as.factor(topic))) +
  geom_col(alpha = 0.8, show.legend = FALSE) +
  facet_wrap(~topic, scales = "free_y") +
  coord_flip() +
  scale_x_reordered() +
  labs(x = NULL, y = expression(beta),
       title = "Highest word probabilities for each topic",
       subtitle = "Different words are associated with different topics") # To see which words contribute the most to each topic
```


```{r gamma, echo=FALSE, message=FALSE, warning=FALSE, fig.width=10}
library(ggthemes)
library(extrafont)
library(scales)

td_gamma_clusters <- tidy(topic_model, matrix = "gamma",                    
                 document_names = rownames(clusters_floodAndIA_sparse))

top_terms_clusters <- td_beta_clusters %>%
  arrange(beta) %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  arrange(-beta) %>%
  select(topic, term) %>%
  summarise(terms = list(term)) %>%
  mutate(terms = map(terms, paste, collapse = ", ")) %>% 
  unnest()

gamma_terms_clusters <- td_gamma_clusters %>%
  group_by(topic) %>%
  summarise(gamma = mean(gamma)) %>%
  arrange(desc(gamma)) %>%
  left_join(top_terms_clusters, by = "topic") %>%
  mutate(topic = paste0("Topic ", topic),
         topic = reorder(topic, gamma))

gamma_terms_clusters %>%
  top_n(22, gamma) %>%
  ggplot(aes(topic, gamma, label = terms, fill = topic, alpha = 0.7)) +
  geom_col(show.legend = FALSE) +
  geom_text(hjust = 0, nudge_y = 0.0005, size = 3,
            family = "CM Roman") +
  coord_flip() +
  scale_y_continuous(expand = c(0,0),
                     limits = c(0, 0.30),
                     labels = percent_format()) +
  theme_tufte(base_family = "CM Roman",ticks = FALSE) +
  theme(plot.title = element_text(size = 14,
                                  family="CM Roman"),
        plot.subtitle = element_text(size = 11),
        legend.position = "none") +
  labs(x = NULL, y = expression(gamma),
       title = "Topics by prevalence in the dataset containing all Colorado tweets",
       subtitle = "With the top words that contribute to each topic")

```

```{r prepare_heatmap, echo=FALSE, message=FALSE}

heat_clusters <- td_gamma_clusters %>% 
  rename(tweetid = document) %>% 
  mutate(tweet_id = as.integer(tweetid)) %>% 
  left_join(colo_clean, by = "tweet_id") %>% 
  select(tweet_id, topic, gamma, day)

# Assigning a topic to each tweet based on which one was had the maximum Gamma
heat_clusters <- heat_clusters %>% 
  group_by(tweet_id) %>% 
  filter(gamma == max(gamma)) %>% 
  ungroup()

# Compute the total number of tweets belonging to each topic each day
heat_clusters <- heat_clusters %>% 
  group_by(day, topic) %>% 
  add_count() %>% 
  rename(total_topic_day = n) %>% 
  ungroup()

# Compute the total number of tweets per day
heat_clusters <- heat_clusters %>% 
  group_by(day) %>% 
  add_count() %>% 
  rename(total_tweets_day = n) %>% 
  mutate(dens = total_topic_day/total_tweets_day)

# Standarazing tweet density (total_topic_day). Computing Z-score values for total_topic_day
scale_this <- function(x){
  (x - mean(x, na.rm=TRUE)) / sd(x, na.rm=TRUE)
}

scaled_heat_clusters <- 
  heat_clusters %>%
  group_by(day) %>%
  mutate(scaled_total_topic_day = scale_this(total_topic_day))

```


```{r gammaclusters2_heatplot, echo=FALSE, message=FALSE}
#Assign color variables
col1 = "#d8e1cf" 
col2 = "#438484"


library(RColorBrewer)
ggplot(scaled_heat_clusters, aes(day, as.factor(topic))) +
  geom_tile(aes(fill = scaled_total_topic_day), color = "white") +
  scale_fill_gradient(low = col1, high = col2) +  
  ylab("Topic") +
  xlab("Flood day") +
  theme(legend.title = element_text(size = 10),
        legend.text = element_text(size = 12),
        plot.title = element_text(size=16),
        axis.title=element_text(size=12,face="bold"),
        axis.text.x = element_text(angle = 90, hjust = 1)) +
  labs(fill = "")
```



```{r joinTweetsTopics, echo=FALSE, message=FALSE}

tweet_and_topic <- dplyr::left_join(heat_clusters, colo_clean, by = "tweet_id") %>% 
  select(tweet_id = tweet_id,
         topic = topic,
         text, text,
         user = user,
         u_profile = u_profile,
         date = date,
         lat = lat,
         lon = lon,
         day = day.x) %>% 
  mutate(topic = paste0("Topic ", topic))

tweet_and_topic$topic <- factor(tweet_and_topic$topic)
```

```{r geoconversion, echo=FALSE, message=FALSE}
tweet_and_topic_geo <- tweet_and_topic %>% 
  st_as_sf(coords = c("lon", "lat"), crs = 4326) #%>% 
  #filter(topic == "Topic 14")
  #filter(user == "WilliamNobleScherer")
  
```

Mapping topics to see spatial distribution:


```{r topicMap, echo=FALSE, message=FALSE, eval=FALSE}
library(colorspace)
library(RColorBrewer)
library(leaflet)
library(htmltools)


markerCol <- colorFactor(palette = brewer.pal(12, "Paired"), tweet_and_topic_geo$topic)

TopicalMap <- leaflet(tweet_and_topic_geo) %>% # Interactive map to see resulting clusters
  addTiles()  %>%
  addProviderTiles(providers$CartoDB.DarkMatter) %>% 
  addCircles(weight = 3, 
             radius=40,
             color= markerCol(tweet_and_topic_geo$topic), 
             stroke = TRUE, 
             fillOpacity = 0.5,
             popup = ~htmlEscape(tweet_and_topic_geo$text))%>% 
  setView(lng = -105, lat = 40, zoom = 8.5)

TopicalMap

#htmlwidgets::saveWidget(TopicalMap, file = "Chapter1/Colorado/Outputs/htmls_generated/MapTopicsFloodAndIAClusters.html")

```

```{r mapping_damage_and_tweets, echo=FALSE, message=FALSE}
library(tmap)

damagePols <- st_read("Data/GroundTruthData/StructuresPols/DamagePolsAllIncluded.geojson")

sometopics <- tweet_and_topic_geo %>% 
  filter(topic == "Topic 8" | topic == "Topic 22")

tmap_mode("view")

map <- tm_shape(damagePols) + 
  tm_polygons(col = "#756bb1",
              alpha = 0.8) +
  tm_shape(sometopics) + 
  tm_dots(col = "topic",
             scale=1,
             alpha = 1)

map + tm_basemap(server = "Stamen.TonerLite")
```


```{r computing_distance, echo=FALSE, message=FALSE}

# Check if there are points within the polygon. St_within also works for this
pnts <- tweet_and_topic_geo %>% 
  mutate(intersection = as.integer(st_intersects(geometry, damagePols))) 

# Compute distances from points to the closest polygon
dist <- geosphere::dist2Line(p = st_coordinates(pnts), line = st_coordinates(damagePols)[,1:2])

# bind results with original points
pts.wit.dist <- cbind(pnts, dist) %>% 
  mutate(distancee = ifelse(is.na(intersection), distance, 0)) %>% 
  select(-c(intersection, distance))


```

```{r plotting_distance, echo=FALSE, message=FALSE, fig.width=10, fig.height=10}

pts.wit.dist %>% 
  ggplot(aes(distancee, date, color = topic)) + # 
  geom_point(size = 2, alpha = 0.7) +
  labs(x = "Distance",
       y = "Date",
       title = "Distance between tweets and nearest reported structure",
       subtitle = "Distance to the nearest reported structure and tweet's date") +
  facet_wrap(~topic) +
  theme(legend.position="bottom")
```

```{r distance_histo, echo=FALSE, message=FALSE}
ggplot(pts.wit.dist, aes(x = distancee)) +
  geom_density(alpha=.2, colour = "#35978f", fill="#35978f")
```

```{r define_distance_classes, echo=FALSE, message=FALSE}
pts.wit.dist <- pts.wit.dist %>%  # The way to define them in a tidyway
  mutate(distClass = ifelse(distancee <= 500, "0-500", 
                        ifelse(distancee <= 1000, "500-1000",
                               ifelse(distancee <= 1500, "1000-1500",
                                      ifelse(distancee <= 2000, "1500-2000",
                                             ifelse(distancee <= 2500, "2000-2500",
                                                    ifelse(distancee <= 3000, "2500-3000",
                                                           ifelse(distancee <= 3500, "3000-3500",
                                                                  ifelse(distancee <= 4000, "3500-4000",
                                                                         ifelse(distancee <= 4500, "4000-4500",
                                                                                ifelse(distancee <= 5000, "4500-5000",
                                                                                       ifelse(distancee <= 5500, "5000-5500",
                                                                                              ifelse(distancee <= 6000, "5500-6000",
                                                                                                     ifelse(distancee <= 6500, "6000-6500", "6500-7000")))))))))))))) %>% 
  mutate(distClass = factor(distClass, levels = unique(distClass))) %>% # Turning stages into factors to use them for plotting later
  mutate(distClass = factor(distClass,levels(distClass)[c(5, 3, 1, 2, 4, 6, 9, 7, 11, 13,10, 12, 14, 8)])) # To plot in an specific order
```

```{r prepare_heatmap, echo=FALSE, message=FALSE}

# Compute the total number of tweets belonging to each topic each day
distancia <- pts.wit.dist %>% 
  group_by(distClass, topic) %>% 
  add_count() %>% 
  rename(total_dist_topic = n) %>% 
  ungroup()

# Compute the total number of tweets per day
distancia <- distancia %>%
  group_by(distClass) %>%
  add_count() %>%
  rename(total_dist = n) %>%
  mutate(dens = total_dist_topic/total_dist)

# Standarazing tweet density (total_topic_day). Computing Z-score values for total_topic_day
scale_this <- function(x){
  (x - mean(x, na.rm=TRUE)) / sd(x, na.rm=TRUE)
}

scaled_distancia <-
  distancia %>%
  group_by(distClass) %>%
  mutate(scaled_total_dist_topic = scale_this(total_dist_topic))

```

```{r distanceCombined, echo=FALSE, message=FALSE, fig.width=10, fig.height=10}
library(viridis)
library(hrbrthemes)

new <- scaled_distancia %>%
                 group_by(distClass, scaled_total_dist_topic, topic, day, dens) %>%
                 summarise(count=n())

new %>%
  ggplot(aes(x=distClass, y=day, size = count, color = topic)) +
    geom_point(alpha=0.7) +
    scale_size(range = c(0.1, 8), name="Population (M)") +
    scale_color_viridis(discrete=TRUE, guide=FALSE) +
    theme_ipsum() +
    theme(legend.position="none") +
    theme(axis.text.x = element_text(size = 8, angle = 90)) +
    facet_wrap(~topic)
```

```{r b,fig.width=14, fig.height=10}

ggplot(scaled_distancia, aes(distancee, color = topic)) + 
  geom_freqpoly(binwidth = 500)+
  xlab("Distance") +
  ylab("Number of tweets by distance band") +
  scale_x_continuous(breaks = seq(0, 6000, 1000), expand = c(0, 0), limits = c(0,NA)) +
  scale_color_viridis(discrete=TRUE, guide=FALSE) +
  theme(legend.title = element_blank(),
        legend.background = element_rect(size = 2),
        legend.text = element_text(size = 10),
        plot.background = element_rect(colour = NA),
        axis.title = element_text(size = 12, face = "bold",
                                  margin=margin(30,20,0,0)),
        axis.line = element_line(colour = "Black"),
        axis.text = element_text(size = 11, color = "black"),
        strip.text = element_text(size = 11),
        legend.key.width = unit(1.5, "cm")) 
# +
#     facet_wrap(~topic)




# geom_freqpoly(data = pts.wit.dist, aes(x = distancee, y = ..count../sum(..count..)), binwidth = 500)

```

