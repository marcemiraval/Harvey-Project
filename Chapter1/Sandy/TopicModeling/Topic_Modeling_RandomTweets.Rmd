---
title: "Topic_Modeling"
author: "Marcela Suarez"
date: "March 31, 2017"
output: html_document
---

```{r setup, include=FALSE}
# install.packages("topicmodels")
# install.packages("slam")
# install.packages("Rmpfr", dependencies = TRUE)
# install.packages("ggplot2", dependencies = TRUE)
# install.packages("tm", dependencies = TRUE)
# installed.packages("SnowballC")
#install.packages("wordcloud", dependencies = TRUE)
library("wordcloud") # Importing package to build the wordcloud
library("RColorBrewer") # Importing package to get palettes for drawing nice plots.
library("tm") # Importing package for text Mining
library("SnowballC")
library("ggplot2") # Importing package for drawing nice plots.
library(topicmodels)
library(tm)
library(slam)
library(Rmpfr)
```


## 1. Description of the dataset

```{r,warning=FALSE, echo=FALSE, cache=FALSE, message=FALSE}
setwd ("/home/marcela/Documents/ICC") 
Sandy <- read.csv("Shelton_et_al_Sandy.csv", header = TRUE, sep = ",") # Loading data into R.
Sandy <- subset(Sandy, Sandy$text!="#Sandy bgz") #Delete rows containing just that text
dim(Sandy) 
```

The dataset contains 99217 from 140780 observations and 13 variables, which corresponds to a 70%. The attributes of this dataset are the following:

```{r,warning=FALSE, echo=FALSE, cache=FALSE, message=FALSE}
names(Sandy) # List the variables in Sandy. 
attach(Sandy) # Making the variables accesible by name within the R session
# is.data.frame(Sandy) # Just checking data type
```

## 2. Exploring Term Frequency

Cleaning up text:
```{r,warning=FALSE, echo=FALSE, cache=FALSE, message=FALSE}
Sandy_list_raw <- Sandy$text # Extracting the text from the tweets in a vector
Sandy_list <- iconv(Sandy_list_raw, to = "utf-8", sub="")
Sandy_list <- tolower(Sandy_list)
Sandy_list  <- gsub("@\\w+", "", Sandy_list) # remove at
Sandy_list <- gsub("http\\w+", "", Sandy_list)  # remove links http
for (j in seq(Sandy_list)){
  Sandy_list[[j]] <- gsub("new york", "New_York", Sandy_list[[j]])
  Sandy_list[[j]] <- gsub("new jersey", "New_Jersey", Sandy_list[[j]])
  Sandy_list[[j]] <- gsub("east coast", "east_coast", Sandy_list[[j]])
  Sandy_list[[j]] <- gsub("stay safe", "stay_safe", Sandy_list[[j]])
}
Sandy_corpus <- Corpus(VectorSource(Sandy_list)) # From here on we start constructing the lexical Corpus:
Sandy_corpus <- tm_map(Sandy_corpus, removePunctuation)
Sandy_corpus <- tm_map(Sandy_corpus, removeNumbers)
Sandy_corpus <- tm_map(Sandy_corpus, stemDocument)
Sandy_corpus <- tm_map(Sandy_corpus, removeWords, stopwords("english"))
```

```{r cars}
dtm <-  DocumentTermMatrix(Sandy_corpus)
dtm <- removeSparseTerms(dtm, 0.99)
term_tfidf <- tapply(dtm$v/row_sums(dtm)[dtm$i],dtm$j, mean) * log2(nDocs(dtm)/col_sums(dtm > 0))
median_tfidf <- summary(term_tfidf)[3]
dtm <- dtm[, term_tfidf >= median_tfidf]
toRemove <- which(row_sums(dtm) == 0,)
dtm <- dtm[row_sums(dtm) > 0,]

harmonicMean <- function(logLikelihoods, precision=2000L) {
llMed <- median(logLikelihoods)
as.double(llMed - log(mean(exp(-mpfr(logLikelihoods,
prec = precision) + llMed))))
}

burnin = 1000
iter = 1000
keep = 50

sequ <- seq(2, 100, 10)

fitted_many <- lapply(sequ, function(k) LDA(dtm, k = k,
method = "Gibbs",control = list(burnin = burnin, iter = iter, keep = keep) ))
# extract logliks from each topic
logLiks_many <- lapply(fitted_many, function(L)
L@logLiks[-c(1:(burnin/keep))])
# compute harmonic means
hm_many <- sapply(logLiks_many, function(h) harmonicMean(h))

plot(sequ, hm_many, type = "l")
k <- sequ[which.max(hm_many)]

seedNum <- 42
lda <- LDA(dtm, k = k, method = "Gibbs", control = list(
burnin = burnin, iter = iter, keep = keep, seed=seedNum))

topTenTermsEachTopic <- terms(lda,10)
View(topTenTermsEachTopic)
write.csv(terms(lda,100), "myTopicModelData.csv")

```
```{r}
ldaplot <- ggplot(data.frame(sequ, hm_many), aes(x=sequ, y=hm_many)) + geom_path(lwd=0.5) +
  theme(text = element_text(family= NULL),
        axis.title.y=element_text(vjust=1, size=8),
        axis.title.x=element_text(vjust=-.7, size=8),
        axis.text=element_text(size=8),
        plot.title=element_text(size=16)) +
  xlab('Number of Topics') +
  ylab('Harmonic Mean') +
  ggtitle(expression(atop("Latent Dirichlet Allocation Analysis of NEN LLIS", atop(italic("How many distinct topics in the abstracts?"), ""))))
ldaplot
```

