---
title: "General-Public Reports About Hurricane Sandy"
author: "Marcela Suarez"
output:
  html_document: default
---


```{r load_packages, include=FALSE, echo=FALSE}
library(tidyverse)
library(sf)
library(lwgeom) #compute area polygon
library(leaflet) #colorBin function
library(htmltools)
library(maptools)
library(markdown)
library(grid)
library(gridExtra)
library(mapview) #sync function
library(classInt)
library(RColorBrewer)
```
# Data Preparation


This study relies on two main datasets. One corresponds to Spotters' reports extracted 
from the historic Storm Event Database from NOAA, which is available online. The second 
corresponds to geotagged Tweets collected and provided by the DOLLY project at the University
of Kentucky. Tweets contained in this dataset were sent from within the United States between
October 24 and October 31 of 2012 with explicit geographic information as latitude and
longitude coordinates.

Spatial information representing state boundaries was downloaded in shapefile format from the
United States Census Bureau website. Polygons representing state boundaries by 2017 are used 
in this study.

Based on this spatial dataset, a polygon representing the states affected by Hurricane Sandy 
was created. According to the SHELDUS database, the following states were affected by the
hurricane: Maryland, Delaware, New Jersey, New York, Connecticut, Massachusetts, Rhode Island,
North Carolina, Virginia, West Virginia, Ohio, Pennsylvania, New Hampshire and District of Columbia.

Based on this information, NWS reports and Tweets sent from these states were selected. We also filtered
only Tweets and reports sent within the two days with more impact reported (Oct/29/2012 and Oct/30/2012), 
which also were the days with reports from both data sources. After filtering, the NWS datasets contains
only 115 reports and the Twitter dataset contains 74807 tweets. 


# Data Analysis

```{r load_prepare_data, include=FALSE, echo=FALSE}

load(file= "Data/sandy_boundary_sf.RData")
load(file = "Data/sandySpotIn_sf.RData")
load(file = "Data/sandyTweetsIn_sf.RData")

#Prepare dataset to be used in function
sandySpotIn_sf <- sandySpotIn_sf %>%
  rename_at("EVENT_ID",~"id")  

```


The purpose of this analysis is to estimate the correspondence between the spatial distribution of Twitter reports and that of reports collected in a traditional fashion by the NWS. Grids of hexagons at a range of sizes are used to compare the overall variance in the density of reports across cells. That density is estimated for each data set as follows: 

***
$\frac{Number\ of\ Reports\ by\ Hexagon}{Total\ Number\ of\ Reports}$


*** 
$\frac{Number\ of\ Tweets\ by\ Hexagon}{Total\ Number\ of\ Tweets}$  

***

In order to vary the cellsize or hexagons size, 32 hexagonal grids covering the area of study with the following different numbers of hexagons were created:
100, 500, 1000, 1500, 2000, 2500, 3000, 3500, 4000, 4500, 5000,
5500, 6000, 6500, 7000, 7500, 8000, 8500, 9000, 9500, 10000, 10500,
11000, 11500, 12000, 12500, 13000, 13500, 14000, 14500, 15000, 15500

```{r list_hexagons_possibilities, include=FALSE, echo=FALSE}

sandy_boundary_area <- st_area(sandy_boundary_sf)

sandy_boundary_sp <-  as(sandy_boundary_sf, "Spatial") # This doesn't go in data preparation because I guess I will use the sf version of the file later.

nHex <- c(h100 = 100, h500 = 500, h1000 = 1000, h1500 = 1500, h2000 = 2000, 
          h2500 = 2500, h3000 = 3000, h3500 = 3500, h4000 = 4000, h4500 = 4500,
          h5000 = 5000, h5500 = 5500, h6000 = 6000, h6500 = 6500, h7000 = 7000, 
          h7500 = 7500, h8000 = 8000, h8500 = 8500, h9000 = 9000, h9500 = 9500, 
          h10000 = 10000, h10500 = 10500, h11000 = 11000, h11500 = 11500, 
          h12000 = 12000, h12500 = 12500, h13000 = 13000, h13500 = 13500, 
          h14000 = 14000, h14500 = 14500, h15000 = 15000, h15500 = 15500)

```

```{r create_hexagonal_grids_sapply, include=FALSE, echo=FALSE}

creaHex <- function(hexNum) {
  sp_hex <- HexPoints2SpatialPolygons(spsample(sandy_boundary_sp, n = hexNum, type = "hexagonal"))
  sf_hex <- st_as_sf(sp_hex) %>%
    mutate(group = 1:nrow(.))
  }

hexagons <- lapply(nHex, creaHex)# Creates a Large list (5 elements, 5.5 Mb)
```

In order to count reports per hexagon, reports information (tweets and NWS reports) was added (joined) to the hexagons.

```{r join_compute_totals, include=FALSE, echo=FALSE}

# Compute the number of hurricane Sandy reports (NWS and Tweets) by hexagon
computeTotals <- function(hex_dfs, reports) {
  hexWithReports <- st_join(hex_dfs, reports) %>%
    group_by(group) %>%
    summarise(total = sum(!is.na(id))) %>%
    mutate(totRatio = total/sum(total))# Add column that normalize totals
}

TotalsSpotters <- lapply(hexagons, computeTotals, reports = sandySpotIn_sf)
TotalsTweets <- lapply(hexagons, computeTotals, reports = sandyTweetsIn_sf)
```

```{r save_datasets, include= FALSE, echo= FALSE}
save(TotalsSpotters, file="Data/TotalsSpotters.Rdata")
save(TotalsTweets, file="Data/TotalsTweets.Rdata")
```

```{r load_totals_datasets, include=FALSE, echo=FALSE}
load(file= "Data/sandy_boundary_sf.RData")
load(file = "Data/sandySpotIn_sf.RData")
```

```{r run_moran_example, include=FALSE, echo=FALSE}

data(oldcol)

coords.OLD <- cbind(COL.OLD$X, COL.OLD$Y)

moran.test(COL.OLD$CRIME, nb2listw(COL.nb, style="W"))
moran.test(COL.OLD$CRIME, nb2listw(COL.nb, style="B"))
moran.test(COL.OLD$CRIME, nb2listw(COL.nb, style="C"))
moran.test(COL.OLD$CRIME, nb2listw(COL.nb, style="S"))
moran.test(COL.OLD$CRIME, nb2listw(COL.nb, style="W"),
 randomisation=FALSE)
colold.lags <- nblag(COL.nb, 3)
moran.test(COL.OLD$CRIME, nb2listw(colold.lags[[2]],
 style="W"))
moran.test(COL.OLD$CRIME, nb2listw(colold.lags[[3]],
 style="W"))
print(is.symmetric.nb(COL.nb))
COL.k4.nb <- knn2nb(knearneigh(coords.OLD, 4))
print(is.symmetric.nb(COL.k4.nb))
moran.test(COL.OLD$CRIME, nb2listw(COL.k4.nb, style="W"))
moran.test(COL.OLD$CRIME, nb2listw(COL.k4.nb, style="W"),
 randomisation=FALSE)
cat("Note: non-symmetric weights matrix, use listw2U()")
moran.test(COL.OLD$CRIME, listw2U(nb2listw(COL.k4.nb,
 style="W")))
moran.test(COL.OLD$CRIME, listw2U(nb2listw(COL.k4.nb,
 style="W")), randomisation=FALSE)
ranks <- rank(COL.OLD$CRIME)
names(ranks) <- rownames(COL.OLD)
moran.test(ranks, nb2listw(COL.nb, style="W"), rank=TRUE)
crime <- COL.OLD$CRIME
is.na(crime) <- sample(1:length(crime), 10)
res <- try(moran.test(crime, nb2listw(COL.nb, style="W"),
 na.action=na.fail))
res
moran.test(crime, nb2listw(COL.nb, style="W"), zero.policy=TRUE,
 na.action=na.omit)
moran.test(crime, nb2listw(COL.nb, style="W"), zero.policy=TRUE,
 na.action=na.exclude)
moran.test(crime, nb2listw(COL.nb, style="W"), na.action=na.pass)
```



```{r export_shps, include=FALSE, echo=FALSE, eval = FALSE}
# Examples exporting files as shapefiles
st_write(hexWithSpoReports, "hexWithSpot.shp", delete_layer = TRUE) # A single file with polygons
st_write(TotalsTweets$h800 , "hexWithTweets.shp800", delete_layer = TRUE) # A polygons spatial data frame from a list
st_write(sandySpot_sf, "sandySpotReports.shp", layer_options = "GEOMETRY=AS_XY", delete_layer = TRUE) # A single files with points
```

```{r map_Sandy_reports, echo=FALSE, eval=FALSE} 
# With EVAL false the map is not loading

SandyReportsMap <- leaflet(sandyTweetsIn_sf) %>% 
  addTiles()  %>%
  addProviderTiles(providers$OpenStreetMap.BlackAndWhite)%>% 
      addCircles(weight = 3, radius=40,
                 color="#ffa500", stroke = TRUE, fillOpacity = 0.8,
                 popup = ~htmlEscape(tweet)) %>%
      addCircles(data = sandySpot_sf, weight = 3, radius=40,
                 color="red", stroke = TRUE, fillOpacity = 0.8,
                 popup = ~htmlEscape(EVENT_NARRATIVE))%>% 
      setView(lng = -74.9, lat = 40.4, zoom = 6)

SandyReportsMap

htmlwidgets::saveWidget(SandyReportsMap, file = "SandyReportsMap2.html")
```

Some of the map comparisons between total of tweets and density of tweets at different cell number levels were created and presented as follows:

#### 100 hexagons covering the area of study

```{r 100hexMaps, warning= FALSE, echo=FALSE}
source("mapReports.R")
mapReports(TotalsTweets$h100)
```

#### 1000 hexagons covering the area of study

```{r 1000hexMaps, warning= FALSE, echo=FALSE}
source("mapReports.R")
mapReports(TotalsTweets$h1000)
```

#### 5500 hexagons covering the area of study

```{r 5500hexMaps, warning= FALSE, echo=FALSE}
## NEXT.Choose the cell size that has the maximum variance.
mapReports(TotalsTweets$h5500)
```

#### 8000 hexagons covering the area of study

```{r 8000hexMaps,warning= FALSE, echo=FALSE}
source("mapReports.R")
mapReports(TotalsTweets$h8000)
```

#### 15500 hexagons covering the area of study

```{r 15500hexMaps,warning= FALSE, echo=FALSE}
source("mapReports.R")
mapReports(TotalsTweets$h15500)
```

```{r sd, include=FALSE, echo=FALSE }
# Getting sd with function
#sd <- hexWithTweets$totRatio %>%
#  sd(.) # this uses n-1 as denominator

# d1 <- data.frame(x = c(1, 5, 6), y = c(2, 4, 3))
# d2 <- data.frame(x = c(3, 5, 6), y = c(7, 4, 3))
# d3 <- data.frame(x = c(8, 15, 6), y = c(12, 14, 3))
# l <- c(d1, d2, d3)
# sd1 <- lapply(l$x, sd)
# Planning the following code as a function

sd100 <- sd(TotalsTweets$h100$totRatio)
sd500 <- sd(TotalsTweets$h500$totRatio)
sd1000  <- sd(TotalsTweets$h1000$totRatio)
sd1500  <- sd(TotalsTweets$h1500$totRatio)
sd2000  <- sd(TotalsTweets$h2000$totRatio)
sd2500  <- sd(TotalsTweets$h2500$totRatio)
sd3000  <- sd(TotalsTweets$h3000$totRatio)
sd3500  <- sd(TotalsTweets$h3500$totRatio)
sd4000  <- sd(TotalsTweets$h4000$totRatio)
sd4500  <- sd(TotalsTweets$h4500$totRatio)
sd5000  <- sd(TotalsTweets$h5000$totRatio)
sd5500  <- sd(TotalsTweets$h5500$totRatio)
sd6000  <- sd(TotalsTweets$h6000$totRatio)
sd6500  <- sd(TotalsTweets$h6500$totRatio)
sd7000  <- sd(TotalsTweets$h7000$totRatio)
sd7500  <- sd(TotalsTweets$h7500$totRatio)
sd8000  <- sd(TotalsTweets$h8000$totRatio)
sd8500  <- sd(TotalsTweets$h8500$totRatio)
sd9000  <- sd(TotalsTweets$h9000$totRatio)
sd9500  <- sd(TotalsTweets$h9500$totRatio)
sd10000  <- sd(TotalsTweets$h10000$totRatio)
sd10500  <- sd(TotalsTweets$h10500$totRatio)
sd11000  <- sd(TotalsTweets$h11000$totRatio)
sd11500  <- sd(TotalsTweets$h11500$totRatio)
sd12000  <- sd(TotalsTweets$h12000$totRatio)
sd12500  <- sd(TotalsTweets$h12500$totRatio)
sd13000  <- sd(TotalsTweets$h13000$totRatio)
sd13500  <- sd(TotalsTweets$h13500$totRatio)
sd14000  <- sd(TotalsTweets$h14000$totRatio)
sd14500  <- sd(TotalsTweets$h14500$totRatio)
sd15000  <- sd(TotalsTweets$h15000$totRatio)
sd15500  <- sd(TotalsTweets$h15500$totRatio)

HexNumber <- c(100, 500, 1000, 1500, 2000, 2500, 3000, 3500, 4000, 4500, 5000,
       5500, 6000, 6500, 7000, 7500, 8000, 8500, 9000, 9500, 10000, 10500,
       11000, 11500, 12000, 12500, 13000, 13500, 14000, 14500, 15000, 15500)

HexDiameter <- sqrt(820861932843/HexNumber) #Estimated

sd_list = c(sd100, sd500, sd1000, sd1500, sd2000, sd2500, sd3000, sd3500, sd4000, 
            sd4500, sd5000,sd5500, sd6000, sd6500, sd7000, sd7500, sd8000, sd8500, 
            sd9000, sd9500, sd10000, sd10500, sd11000, sd11500, sd12000, sd12500, 
            sd13000, sd13500,sd14000, sd14500, sd15000, sd15500)

sd_df <- data.frame(x = HexDiameter, y = sd_list)
```

***
#### Hexagons' diameter versus overall standard deviation in the density of reports. Hexagons' diameters were calculated based on the range of number of hexagons covering the area of study. 

```{r sd_plot, warning= FALSE, echo=FALSE }
sd_plot <- ggplot(sd_df, aes(x = HexDiameter, y = sd_list)) +
  geom_line() + 
  geom_point(size = 1, color = "#dd1c77") +
  labs(x = "Hexagons' Diameter (m)", y = "sd")
  
sd_plot
```
