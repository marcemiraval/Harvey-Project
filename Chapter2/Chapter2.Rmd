---
title: "General-Public Reports About Hurricane Sandy"
author: "Marcela Suarez"
output:
  html_document: default
---


```{r load_packages, include=FALSE, echo=FALSE}

library(tidyverse)
library(sf)
library(leaflet)
library(htmltools)
library(maptools)
library(markdown)
library(grid)
library(gridExtra)
library(mapview)
library(classInt)
library(RColorBrewer)
```
# Data Preparation


This study relies on two main datasets. One corresponds to Spotters' reports extracted 
from the historic Storm Event Database from NOAA, which is available online. The second 
corresponds to geotagged Tweets collected and provided by the DOLLY project at the University
of Kentucky. Tweets contained in this dataset were sent from within the United States between
October 24 and October 31 of 2012 with explicit geographic information as latitude and
longitude coordinates.

Spatial information representing state boundaries was downloaded in shapefile format from the
United States Census Bureau website. Polygons representing state boundaries by 2017 are used 
in this study.

Based on this spatial dataset, a polygon representing the states affected by Hurricane Sandy 
was created. According to the SHELDUS database, the following states were affected by the
hurricane: Maryland, Delaware, New Jersey, New York, Connecticut, Massachusetts, Rhode Island,
North Carolina, Virginia, West Virginia, Ohio, Pennsylvania, New Hampshire and District of Columbia.

Based on this information, NWS reports and Tweets sent from these states were selected. We also filtered
only Tweets and reports sent within the two days with more impact reported (Oct/29/2012 and Oct/30/2012), 
which also were the days with reports from both data sources. After filtering, the NWS datasets contains
only 115 reports and the Twitter dataset contains 74807 tweets. 


# Data Analysis

```{r load_prepare_data, include=FALSE, echo=FALSE}

load(file= "Data/sandy_boundary_sf.RData")
load(file = "Data/sandySpotIn_sf.RData")
load(file = "Data/sandyTweetsIn_sf.RData")

#Prepare dataset to be used in function
sandySpotIn_sf <- sandySpotIn_sf %>%
  rename_at("EVENT_ID",~"id")  

```


The purpose of this analysis is to estimate the correspondence between the spatial distribution of Twitter reports and that of reports collected in a traditional fashion by the NWS. Grids of hexagons at a range of sizes are used to compare the overall variance in the density of reports across cells. That density is estimated for each data set as follows: 

***
$\frac{Number\ of\ Reports\ by\ Hexagon}{Total\ Number\ of\ Reports}$


*** 
$\frac{Number\ of\ Tweets\ by\ Hexagon}{Total\ Number\ of\ Tweets}$  


```{r list_hexagons_possibilities, include=FALSE, echo=FALSE}

sandy_boundary_sp <-  as(sandy_boundary_sf, "Spatial") # This doesn't go in data preparation because I guess I will use the sf version of the file later.
nHex <- c(h1000 = 1000, h1500 = 1500, h2000 = 2000, h2500 = 2500, h3000 = 3000,
          h3500 = 3500, h4000 = 4000, h4500 = 4500, h5000 = 5000, h5500 = 5500)

```

```{r create_hexagonal_grids_sapply, include=FALSE, echo=FALSE}

creaHex <- function(hexNum) {
  sp_hex <- HexPoints2SpatialPolygons(spsample(sandy_boundary_sp, n = hexNum, type = "hexagonal"))
  sf_hex <- st_as_sf(sp_hex) %>%
    mutate(group = 1:nrow(.))
  }

hexagons <- lapply(nHex, creaHex)# Creates a Large list (5 elements, 5.5 Mb)

# ggplot() + geom_sf(data = hexagons$h800) # To test lapply function
```

I get all counts of tweets based on hexagons definition. So I am adding reports information to hexagon information, not the opposite.

```{r join_compute_totals, include=FALSE, echo=FALSE}

# Compute the number of hurricane Sandy reports (NWS and Tweets) by hexagon
computeTotals <- function(hex_dfs, reports) {
  hexWithReports <- st_join(hex_dfs, reports) %>%
    group_by(group) %>%
    summarise(total = sum(!is.na(id))) %>%
    mutate(totRatio = total/sum(total))# Add column that normalize totals
}

TotalsSpotters <- lapply(hexagons, computeTotals, reports = sandySpotIn_sf)
TotalsTweets <- lapply(hexagons, computeTotals, reports = sandyTweetsIn_sf)
```

```{r export_shps, include=FALSE, echo=FALSE, eval = FALSE}
# Examples exporting files as shapefiles
st_write(hexWithSpoReports, "hexWithSpot.shp", delete_layer = TRUE) # A single file with polygons
st_write(TotalsTweets$h800 , "hexWithTweets.shp800", delete_layer = TRUE) # A polygons spatial data frame from a list
st_write(sandySpot_sf, "sandySpotReports.shp", layer_options = "GEOMETRY=AS_XY", delete_layer = TRUE) # A single files with points
```

```{r map_Sandy_reports, echo=FALSE, eval=FALSE} 
# With EVAL false the map is not loading

SandyReportsMap <- leaflet(sandyTweetsIn_sf) %>% 
  addTiles()  %>%
  addProviderTiles(providers$OpenStreetMap.BlackAndWhite)%>% 
      addCircles(weight = 3, radius=40,
                 color="#ffa500", stroke = TRUE, fillOpacity = 0.8,
                 popup = ~htmlEscape(tweet)) %>%
      addCircles(data = sandySpot_sf, weight = 3, radius=40,
                 color="red", stroke = TRUE, fillOpacity = 0.8,
                 popup = ~htmlEscape(EVENT_NARRATIVE))%>% 
      setView(lng = -74.9, lat = 40.4, zoom = 6)

SandyReportsMap

htmlwidgets::saveWidget(SandyReportsMap, file = "SandyReportsMap2.html")
```

```{r}
#HexMapsSpotters <- lapply(TotalsSpotters, mapReports)
mapReports(TotalsTweets$h1000)

```


```{r, warning= FALSE, echo=FALSE}
## NEXT.Choose the cell size that has the maximum variance.
mapReports(TotalsTweets$h5500)
```

